{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7260a3b2",
   "metadata": {},
   "source": [
    "# Python for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf1fa",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Supervised Machine Learning**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup our Python environment and get started with our Explore California supervised machine learning exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e1c66",
   "metadata": {},
   "source": [
    "### Practice Exercises\n",
    "\n",
    "1. Ingest and explore our 2 datasets for product recommendation and purchase prediction\n",
    "2. Implement `scikit-learn` pipelines for multi-class classification - product recommendations\n",
    "3. Implement `scikit-learn` pipelines for binary classification - purchase predictions\n",
    "4. Investigate model outputs and explanations using `SHAP` and `LIME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd55533",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or hit `command/shift + enter` to run individual cells one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install --quiet lightgbm==4.6.0\n",
    "!pip install --quiet optuna==4.4.0\n",
    "!pip install --quiet lime==0.2.0.1\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259.git data\n",
    "\n",
    "# Need to change directory into \"data\" to download git lfs data objects\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct\n",
    "%cd ..\n",
    "\n",
    "# Load in some Python Libraries\n",
    "# =============================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# We'll use a style that is colorblind-friendly\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Turn off warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b60251",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "Let's first load in our `product_recommendation` and `purchase_prediction` datasets using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our datasets\n",
    "product_recommendation_df = pd.read_csv(\"data/product_recommendation.csv\")\n",
    "purchase_prediction_df = pd.read_csv(\"data/purchase_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa4f11",
   "metadata": {},
   "source": [
    "## 1.1 Multi-Class Classification\n",
    "\n",
    "Let‚Äôs start by exploring our `product_recommendation_df`. We‚Äôll be treating this as a **multi-class classification** problem, where the goal is to predict which product a user is most likely to purchase based on their individual attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92600b4",
   "metadata": {},
   "source": [
    "## 1.1.1 Exploratory Data Analysis\n",
    "\n",
    "Before we build any models, it‚Äôs important to take some time to understand the structure and quality of our dataset. In this section, we‚Äôll perform a few key data exploration steps:\n",
    "\n",
    "- ‚úÖ Preview the first few rows to understand the data format and feature names\n",
    "- üìä Check the distribution of our target variable `product_name` to see how balanced the classes are\n",
    "- üîç Look for any missing values or data quality issues\n",
    "- üß† Review the number and types of features available ‚Äî in this case, we‚Äôre working with binary indicators (0 or 1)\n",
    "\n",
    "These simple checks help us catch problems early and set up our classification pipeline with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "product_recommendation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "product_recommendation_df['product_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9dd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "# We can sort the results to see which columns have the most missing values\n",
    "product_recommendation_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature data types\n",
    "product_recommendation_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of feature columns (excluding ID and target)\n",
    "feature_cols = product_recommendation_df.columns.difference(['user_id', 'product_name'])\n",
    "\n",
    "# Check if all values are binary (0 or 1)\n",
    "non_binary = [\n",
    "    col for col in feature_cols\n",
    "    if not product_recommendation_df[col].isin([0, 1]).all()\n",
    "]\n",
    "\n",
    "print(\"Non-binary columns:\", non_binary if non_binary else \"‚úÖ All features are binary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44c68f",
   "metadata": {},
   "source": [
    "### 1.1.2 Reusable EDA Function\n",
    "\n",
    "Before training any machine learning model, it‚Äôs essential to understand your dataset. To make this easier and repeatable, we can create a reusable helper function called `explore_multiclass_dataset()`\n",
    "\n",
    "This function performs our same key exploratory data analysis (EDA) steps as above, but this time, tailored for generic **multi-class classification problems**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_multiclass_dataset(df, target_col='product_name', id_col='user_id'):\n",
    "    \"\"\"\n",
    "    Perform basic exploratory data analysis (EDA) for a multi-class classification dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataset containing a multi-class target column and feature columns.\n",
    "\n",
    "    target_col : str, default='product_name'\n",
    "        The name of the target column to predict. This should be a categorical feature with more than two unique classes.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of a unique identifier column to exclude from binary feature validation.\n",
    "\n",
    "    This function performs:\n",
    "    ------------------------\n",
    "    - A preview of the first few rows of the dataset\n",
    "    - A count and bar plot of the target class distribution\n",
    "    - A missing value check across all columns\n",
    "    - A summary of feature data types\n",
    "    - A validation that all non-ID, non-target features are binary indicators (0 or 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------\n",
    "    # Preview the dataset\n",
    "    # --------------------\n",
    "    print(\"üìå Preview of dataset:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # --------------------\n",
    "    # Show class distribution of the target column\n",
    "    # --------------------\n",
    "    print(\"\\nüìä Target variable distribution:\")\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    display(class_counts)\n",
    "\n",
    "    # --------------------\n",
    "    # Plot class distribution bar chart\n",
    "    # --------------------\n",
    "    print(\"\\nüìâ Class distribution plot:\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.bar(class_counts.index, class_counts.values, color='skyblue')\n",
    "    ax.set_title(\"Product Class Distribution\")\n",
    "    ax.set_xlabel(\"Product Name\")\n",
    "    ax.set_ylabel(\"Number of Purchases\")\n",
    "    ax.set_xticks(range(len(class_counts)))\n",
    "    ax.set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------\n",
    "    # Check for missing values\n",
    "    # --------------------\n",
    "    print(\"\\nüîç Missing values per column:\")\n",
    "    display(df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "    # --------------------\n",
    "    # Show data type breakdown\n",
    "    # --------------------\n",
    "    print(\"\\nüß† Feature type summary:\")\n",
    "    display(df.dtypes.value_counts())\n",
    "\n",
    "    # --------------------\n",
    "    # Binary feature validation\n",
    "    # --------------------\n",
    "    print(\"\\n‚úÖ Binary feature check (excluding ID and target):\")\n",
    "    feature_cols = df.columns.difference([id_col, target_col])\n",
    "    non_binary = [\n",
    "        col for col in feature_cols\n",
    "        if not df[col].isin([0, 1]).all()\n",
    "    ]\n",
    "\n",
    "    if non_binary:\n",
    "        print(\"‚ö†Ô∏è Non-binary columns detected:\", non_binary)\n",
    "    else:\n",
    "        print(\"‚úÖ All features are binary (0 or 1).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e259e4",
   "metadata": {},
   "source": [
    "### 1.1.3 Using our EDA Function\n",
    "\n",
    "You can also apply this function to **any dataset** that follows this structure:\n",
    "\n",
    "- A **target column** with multiple class labels (e.g. product names, categories)\n",
    "- One or more **feature columns**, ideally binary indicators (0 or 1)\n",
    "- An optional **ID column** used to uniquely identify rows (not used for modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50333afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function for our product_recommendation_df dataset\n",
    "explore_multiclass_dataset(\n",
    "  product_recommendation_df,  # Input dataframe\n",
    "  target_col='product_name',  # Default target column\n",
    "  id_col='user_id'            # Default ID column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66bc1a",
   "metadata": {},
   "source": [
    "## 1.3 Binary Classification\n",
    "\n",
    "Now let‚Äôs explore our `purchase_prediction_df`. In this case, we‚Äôre working on a **binary classification** problem, where the goal is to predict whether a user will make a **tour product purchase** based on their individual attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd29ad",
   "metadata": {},
   "source": [
    "### 1.3.1 Binary Classification EDA Function\n",
    "\n",
    "To streamline our exploratory data analysis for binary classification problems, we‚Äôve created a reusable Python function: `explore_binary_dataset()`.\n",
    "\n",
    "This function helps you quickly understand the structure, balance, and quality of your dataset ‚Äî and is especially useful for previewing training data before fitting a machine learning model.\n",
    "\n",
    "Here‚Äôs what it does:\n",
    "\n",
    "- üìå **Preview the dataset** with the first few rows  \n",
    "- üìä **Inspect the distribution** of your binary target variable (e.g. 0 vs 1)  \n",
    "- üîÑ **Break down labels by dataset split** (e.g. train, validation, test)  \n",
    "- üìâ **Visualize class balance** using bar charts and stacked plots  \n",
    "- üìà **Automatically detect numeric features** and show histograms (up to 10 by default)  \n",
    "- ‚úÖ **Check binary indicator columns** for modeling readiness  \n",
    "- üîç **Highlight missing values** and feature type summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_binary_dataset(df, target_col='label', id_col='user_id', period_order=['train', 'validation', 'test'], max_numeric=10):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis (EDA) for a binary classification dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataset containing features, a binary target column, and optional metadata.\n",
    "\n",
    "    target_col : str, default='label'\n",
    "        The name of the binary target column (should contain only 0s and 1s).\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of the column used as a unique identifier for each row.\n",
    "        This will be excluded from feature analysis.\n",
    "\n",
    "    period_order : list of str, default=['train', 'validation', 'test']\n",
    "        The desired order of dataset partitions (useful for plotting label distributions by period).\n",
    "\n",
    "    max_numeric : int, default=10\n",
    "        Maximum number of numeric (non-binary) columns to explore with summary stats and histograms.\n",
    "\n",
    "    This function will:\n",
    "    -------------------\n",
    "    - Preview the first few rows of the dataset\n",
    "    - Show the distribution of the binary target\n",
    "    - Display label counts and proportions by `period` (if available)\n",
    "    - Visualize class balance (bar charts and stacked bar plots)\n",
    "    - Identify and summarize numeric (non-binary) columns\n",
    "    - Limit numeric summaries to `max_numeric`\n",
    "    - Identify binary columns\n",
    "    - Show missing value counts per column\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------\n",
    "    # Dataset Preview\n",
    "    # --------------------\n",
    "    print(\"üìå Preview of dataset:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # --------------------\n",
    "    # Target Label Distribution\n",
    "    # --------------------\n",
    "    print(f\"\\nüìä Distribution of binary target `{target_col}`:\")\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    display(target_counts)\n",
    "\n",
    "    # --------------------\n",
    "    # Label Distribution by Period\n",
    "    # --------------------\n",
    "    print(\"\\nüìä Target label distribution by `period`:\")\n",
    "    label_by_period = (\n",
    "        df.groupby(\"period\")[target_col]\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(period_order)\n",
    "    )\n",
    "    display(label_by_period)\n",
    "\n",
    "    print(\"\\nüìä Proportions of each label by `period`:\")\n",
    "    label_proportions = label_by_period.div(label_by_period.sum(axis=1), axis=0)\n",
    "    display(label_proportions)\n",
    "\n",
    "    # --------------------\n",
    "    # Plot: Overall Class Balance\n",
    "    # --------------------\n",
    "    print(\"\\nüìâ Overall target class distribution plot:\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    bars = ax.bar(target_counts.index.astype(str), target_counts.values)\n",
    "    ax.set_title(\"Binary Target Distribution\")\n",
    "    ax.set_xlabel(\"Label\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height + 200, f'{int(height)}',\n",
    "                ha='center', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------\n",
    "    # Plot: Grouped Bar Chart by Period\n",
    "    # --------------------\n",
    "    print(\"\\nüìä Label breakdown by period (grouped bar chart):\")\n",
    "    label_by_period.plot(kind='bar', figsize=(8, 5), stacked=False)\n",
    "    plt.title(\"Label Counts by Period\")\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------\n",
    "    # Plot: Normalized Stacked Bar Chart by Period\n",
    "    # --------------------\n",
    "    print(\"\\nüìä Label proportions by period (normalized stacked bar chart):\")\n",
    "    label_proportions.plot(kind='bar', stacked=True, figsize=(8, 5))\n",
    "    plt.title(\"Label Proportions by Period\")\n",
    "    plt.xlabel(\"Period\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Label', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------\n",
    "    # Identify Binary vs Numeric Columns\n",
    "    # --------------------\n",
    "    exclude_cols = [id_col, target_col, 'label_date', 'period']\n",
    "    working_df = df.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "    # Numeric = >2 unique numeric values\n",
    "    numeric_cols = [\n",
    "        col for col in working_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        if df[col].nunique(dropna=True) > 2\n",
    "    ]\n",
    "\n",
    "    # Binary = all values in [0, 1] (excluding NaNs)\n",
    "    binary_cols = [\n",
    "        col for col in working_df.columns\n",
    "        if df[col].dropna().isin([0, 1]).all()\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüî¢ Feature Type Summary:\")\n",
    "    print(f\"- Binary columns: {len(binary_cols)}\")\n",
    "    print(f\"- Numeric (non-binary) columns: {len(numeric_cols)}\")\n",
    "\n",
    "    # --------------------\n",
    "    # Plot Numeric Feature Histograms\n",
    "    # --------------------\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nüìà Exploring up to {max_numeric} numeric column(s):\")\n",
    "        for col in numeric_cols[:max_numeric]:\n",
    "            print(f\"\\nüìà Distribution of numeric column `{col}`:\")\n",
    "            display(df[col].describe())\n",
    "            df[col].hist(bins=30, edgecolor='black', figsize=(8, 4))\n",
    "            plt.title(f\"Distribution of `{col}`\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        if len(numeric_cols) > max_numeric:\n",
    "            print(f\"‚ö†Ô∏è {len(numeric_cols) - max_numeric} additional numeric columns were skipped. \"\n",
    "                  f\"Increase `max_numeric` to show more.\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è No numeric (non-binary) features found.\")\n",
    "\n",
    "    # --------------------\n",
    "    # Check for Missing Values\n",
    "    # --------------------\n",
    "    print(\"\\nüîç Missing values per column:\")\n",
    "    display(df.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b30cb6",
   "metadata": {},
   "source": [
    "### 1.3.2 Using Our Binary EDA Function\n",
    "\n",
    "You can apply this function to your **own binary classification dataset** by passing in:\n",
    "\n",
    "- `df`: your `pandas` DataFrame  \n",
    "- `target_col`: the name of your binary label column (e.g. `'churned'`, `'clicked'`, `'purchased'`)  \n",
    "- `id_col`: a unique identifier (e.g. `'user_id'`, `'session_id'`) to exclude from feature checks  \n",
    "- *(optional)* `period_order`: how to order partitions like `'train'`, `'validation'`, and `'test'`  \n",
    "- *(optional)* `max_numeric`: how many numeric features to explore (default = 10)\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è If your dataset has many numeric columns, only the first 10 will be visualized. You can increase this limit using the `max_numeric` argument if needed.\n",
    "\n",
    "This function is designed to be flexible ‚Äî just make sure your target variable contains **only two values (0 and 1)**, and you‚Äôll be ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ace1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_binary_dataset(\n",
    "  purchase_prediction_df,\n",
    "  target_col='label',  # Default target column\n",
    "  id_col='user_id',    # Default ID column\n",
    "  period_order=['train', 'validation', 'test']  # Default period order\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491181f",
   "metadata": {},
   "source": [
    "# 2. Multi-Class Classification\n",
    "\n",
    "Now that we‚Äôve explored our datasets, it‚Äôs time to build our first **end-to-end machine learning pipelines** using `scikit-learn`.\n",
    "\n",
    "We‚Äôll begin by focusing on our **multi-class classification task** ‚Äî where the goal is to predict which product a user is most likely to purchase based on their binary attribute profile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa751c",
   "metadata": {},
   "source": [
    "## 2.1 ML Pipelines Overview\n",
    "\n",
    "In this section, we‚Äôll:\n",
    "\n",
    "- ‚öôÔ∏è Build an initial pipeline that includes preprocessing and model fitting  \n",
    "- üîÅ Explore how to **swap in different classifiers** (e.g. Logistic Regression, Random Forest, LightGBM)  \n",
    "- üß™ Incorporate **cross-validation and hyperparameter tuning**  \n",
    "- üìä Use evaluation tools like the **confusion matrix** and `classification_report` to assess performance  \n",
    "\n",
    "We'll take an **incremental approach**, starting with a minimal setup and adding complexity step-by-step ‚Äî so you can clearly see how each part of the ML pipeline contributes to model performance and interpretability.\n",
    "\n",
    "Later in the tutorial, we‚Äôll **apply a similar pipeline structure** ‚Äî with a few key adjustments ‚Äî to tackle our **binary classification problem**, where the goal is to predict whether a user is likely to make a purchase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fab57b",
   "metadata": {},
   "source": [
    "### 2.1.1 Function Scope and Reusability Tips\n",
    "\n",
    "You might notice in the following cells that all the `scikit-learn` packages are imported *within* the function body ‚Äî rather than at the top of the notebook.\n",
    "\n",
    "This is intentional! By importing modules like `LogisticRegression` and `train_test_split` **inside the function**, we keep those dependencies **scoped only to the function itself**.\n",
    "\n",
    "This has a few advantages:\n",
    "\n",
    "- ‚úÖ Keeps your global namespace clean ‚Äî especially in shared or interactive notebooks  \n",
    "- üì¶ Makes the function more portable if you want to copy or reuse it elsewhere  \n",
    "- üß™ Helps isolate your logic during testing or modular development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199b982",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.2 Saving the Model for Deployment or Reuse\n",
    "\n",
    "Our function implementations also returns a dictionary that includes:\n",
    "\n",
    "- The **trained model**\n",
    "- The **label encoder** (for decoding predictions)\n",
    "- The train/validation splits (useful for further evaluation)\n",
    "- A list of the original **feature names**\n",
    "\n",
    "These outputs can be reused in downstream tasks such as:\n",
    "\n",
    "- Generating predictions for new users  \n",
    "- Visualizing model performance in a **dashboard**  \n",
    "- Embedding the model into an **AI application or workflow**  \n",
    "- Saving to disk using `joblib` or `pickle` for later deployment\n",
    "\n",
    "For example, to save model artefacts to disk - we could run the following in a Python cell:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(results[\"model\"], \"logreg_model.pkl\")\n",
    "joblib.dump(results[\"label_encoder\"], \"label_encoder.pkl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c900f",
   "metadata": {},
   "source": [
    "## 2.2 Logistic Regression ML Pipeline\n",
    "\n",
    "In this step, we‚Äôll train a simple **multiclass logistic regression model** to predict which tour product a user is most likely to purchase ‚Äî based on their individual attributes.\n",
    "\n",
    "The `train_multiclass_logistic_regression_model()` function performs the following:\n",
    "\n",
    "- üì¶ Splits the dataset into training and validation sets (using stratified sampling to preserve class balance)\n",
    "- üî† Encodes the target labels using scikit-learn‚Äôs `LabelEncoder`\n",
    "- ‚öôÔ∏è Trains a logistic regression model on the training data\n",
    "- üìä Evaluates the model using `classification_report`, showing precision, recall, and F1-score per class\n",
    "- üìÅ Returns a dictionary of useful components: the model, data splits, label encoder, and feature names\n",
    "\n",
    "**‚úÖ How to Use the Function**\n",
    "\n",
    "You can apply this function to any dataset with:\n",
    "- A categorical target column (e.g. tour product names)\n",
    "- A set of binary indicator features (0 or 1)\n",
    "- An optional ID column used to uniquely identify rows\n",
    "\n",
    "To adjust model behavior:\n",
    "- Use parameters like `penalty`, `C`, and `solver` to apply **Lasso (L1)**, **Ridge (L2)**, or **ElasticNet** regularization\n",
    "- Use `class_weight='balanced'` to automatically adjust for **class imbalance** in your multi-class classification tasks ‚Äî this helps ensure that minority classes are not ignored during training\n",
    "- Modify `test_size` or `random_state` to control how your validation set is split\n",
    "- Specify `drop_cols` to exclude any additional metadata columns from modeling\n",
    "\n",
    "This gives us a solid foundation to experiment with different models, tune hyperparameters, and eventually deploy our trained classifier into a real-world AI workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_logistic_regression_model(\n",
    "    df,\n",
    "    target_col='product_name',\n",
    "    id_col='user_id',\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a logistic regression model on the Explore California product recommendation dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing user attributes and a multi-class target label.\n",
    "\n",
    "    target_col : str, default='product_name'\n",
    "        The name of the multi-class target column.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of the unique identifier column (excluded from training features).\n",
    "\n",
    "    drop_cols : list of str, optional\n",
    "        Additional columns to exclude from training (e.g., ['product_name', 'user_id']).\n",
    "        If None, will default to [target_col, id_col].\n",
    "\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Controls shuffling for reproducibility in the train/test split.\n",
    "\n",
    "    class_weight : str, dict, or None, default='balanced'\n",
    "        Weights associated with classes. Useful when classes are imbalanced.\n",
    "        - Use `'balanced'` to automatically adjust weights inversely proportional to class frequencies.\n",
    "        - You can also provide a custom dictionary of weights, or use `None` for no weighting.\n",
    "\n",
    "    penalty : str, default='l2'\n",
    "        Type of regularization to apply:\n",
    "        - 'l2' for Ridge\n",
    "        - 'l1' for Lasso\n",
    "        - 'elasticnet' for a mix (requires solver='saga')\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Inverse of regularization strength. Smaller values imply stronger regularization.\n",
    "\n",
    "    solver : str, default='lbfgs'\n",
    "        Algorithm to use in the optimization problem.\n",
    "        - 'liblinear' supports L1\n",
    "        - 'saga' supports elasticnet\n",
    "\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations for solver to converge.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the trained model, label encoder, and training/validation splits.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Select features and target\n",
    "    # ---------------------------------------\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [target_col, id_col]\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # üî† Encode the target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # üîÄ Create train/validation split\n",
    "    # You can adjust test_size or stratify behavior here\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X,\n",
    "        y_encoded,\n",
    "        test_size=test_size,\n",
    "        stratify=y_encoded,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # üì¶ Train logistic regression model\n",
    "    # You can modify penalty, C, solver, or class_weight to control behavior:\n",
    "    # - class_weight='balanced' is recommended for class imbalance\n",
    "    # - penalty='l2' ‚Üí Ridge\n",
    "    # - penalty='l1' ‚Üí Lasso (requires solver='liblinear')\n",
    "    # - penalty='elasticnet' ‚Üí L1 + L2 mix (requires solver='saga')\n",
    "    model = LogisticRegression(\n",
    "        class_weight=class_weight,\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # üìä Evaluate the model using precision, recall, and F1 score\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(classification_report(y_val, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # Return all components for downstream use or export\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb685d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take ~30 seconds to run!\n",
    "product_recommendations_logistic_regression = train_multiclass_logistic_regression_model(product_recommendation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's also demonstrate how to save the model artefacts using joblib\n",
    "# This is useful for later deployment or inference!\n",
    "import joblib\n",
    "\n",
    "# Choose only certain components parts of the model output\n",
    "pickle_outputs = [\"model\", \"label_encoder\", \"attribute_names\"]\n",
    "filtered_outputs = {key: product_recommendations_logistic_regression[key] for key in pickle_outputs}\n",
    "\n",
    "# Write it out as a pickle Python dictionary object\n",
    "joblib.dump(filtered_outputs, \"logistic_model_outputs.pkl\")\n",
    "\n",
    "# This is how we can load it back into our Notebook scope!\n",
    "model_artefacts = joblib.load(\"logistic_model_outputs.pkl\")\n",
    "\n",
    "# Refer to each element within the object\n",
    "ml_model = model_artefacts[\"model\"]\n",
    "ml_label_encoder = model_artefacts[\"label_encoder\"]\n",
    "ml_attribute_names = model_artefacts[\"attribute_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0699d",
   "metadata": {},
   "source": [
    "## 2.3 Model Performance Summary\n",
    "\n",
    "Our logistic regression model achieved **overall accuracy of 97%** across 10 product classes in the validation set.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- ‚úÖ **High-performing classes** include:\n",
    "  - *California Classics*, *Southern Skies Trail*, and *Sun & Stone Discovery* ‚Äî all with precision and recall above 95%.\n",
    "  \n",
    "- ‚ö†Ô∏è **Lower-performing but small classes**:\n",
    "  - *Epic NorCal Expedition*, *Giant Trees & Granite Dreams*, and *Golden State Wonders* had **lower precision** (~70‚Äì75%) but **very high recall** (98‚Äì99%), meaning the model tends to over-predict these classes slightly but still catches most true cases.\n",
    "\n",
    "- üìê **Macro average F1-score** is **0.93**, indicating solid average performance across all classes.\n",
    "- ‚öñÔ∏è **Weighted average F1-score** is **0.97**, showing strong results especially for more frequent classes.\n",
    "\n",
    "This strong baseline provides a great starting point for experimenting with other models and tuning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26918792",
   "metadata": {},
   "source": [
    "## 2.4 Visualizing Model Performance with a Confusion Matrix\n",
    "\n",
    "Once we‚Äôve trained our multi-class logistic regression model, it‚Äôs important to go beyond just accuracy scores and look at **where** our model is getting predictions right ‚Äî and where it‚Äôs going wrong.\n",
    "\n",
    "A **confusion matrix** gives us a detailed breakdown of how often each class (in our case, tour product) is correctly predicted vs. confused with others. This can reveal:\n",
    "\n",
    "- Which classes the model handles confidently and accurately ‚úÖ  \n",
    "- Which classes tend to get confused with others ü§î  \n",
    "- Whether certain products are underrepresented or difficult to classify ‚ö†Ô∏è  \n",
    "\n",
    "For example, if two tours are similar in theme or user preferences, the model might struggle to differentiate between them ‚Äî and this would show up as off-diagonal values in the matrix.\n",
    "\n",
    "We‚Äôll implement a reusable `plot_confusion_matrix()` function to help us visualize this breakdown.  \n",
    "It takes in the model output dictionary and displays a color-coded matrix with prediction vs. actual classes.\n",
    "\n",
    "> This step helps guide future improvements ‚Äî such as adjusting class weights, rebalancing data, or engineering new features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model_output, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot a labeled confusion matrix with both raw counts and row-normalized percentages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_output : dict\n",
    "        Dictionary returned from a model training function (e.g., `train_multiclass_logistic_regression_model`)\n",
    "        that must include:\n",
    "        - 'model': Trained scikit-learn classifier\n",
    "        - 'X_val': Validation feature matrix\n",
    "        - 'y_val': Encoded ground truth labels for validation set\n",
    "        - 'label_encoder': Fitted LabelEncoder used to transform the target\n",
    "\n",
    "    title : str, default=\"Confusion Matrix ‚Äì Multiclass Classification\"\n",
    "        Custom title to show at the top of the plot\n",
    "\n",
    "    This function will:\n",
    "    -------------------\n",
    "    - Predict outcomes on validation data using the trained model\n",
    "    - Compute the confusion matrix (true vs. predicted labels)\n",
    "    - Normalize the matrix row-wise to calculate per-class prediction proportions\n",
    "    - Plot a heatmap showing both raw counts and normalized percentages in each cell\n",
    "    - Label axes with the decoded class names for easy interpretation\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Use this plot to visually assess where your model is performing well or making systematic errors.\n",
    "    - Works best for multi-class classification tasks, especially when paired with imbalanced data.\n",
    "    - This version includes both absolute values and percentages in each cell to aid interpretation.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # üîç Extract components from model output\n",
    "    model = model_output[\"model\"]\n",
    "    X_val = model_output[\"X_val\"]\n",
    "    y_val = model_output[\"y_val\"]\n",
    "    label_encoder = model_output[\"label_encoder\"]\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # üîÆ Generate predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # üìä Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e., by the total number of true labels per class)\n",
    "    # This allows us to calculate what percentage of actual class 'i' was predicted as class 'j'\n",
    "    # The result is a matrix of proportions, where each row sums to 1 (or 100%)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # üé® Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # cividis is useful for colorblind-friendly visualizations\n",
    "    # You can change the colormap to 'Blues', 'Greens' or other options\n",
    "    im = ax.imshow(cm_normalized, interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # üè∑Ô∏è Axis labels and ticks\n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_title(title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # üî¢ Annotations (aligned to cell centers)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            percent = cm_normalized[i, j] * 100\n",
    "            text = f\"{count}\\n{percent:.1f}%\"\n",
    "            ax.text(j, i, text, ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "  product_recommendations_logistic_regression,\n",
    "  \"Logistic Regression - Product Recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb605b8",
   "metadata": {},
   "source": [
    "## 2.3 Random Forest with Cross-Validation and Hyperparameter Search\n",
    "\n",
    "In this step, we‚Äôll train a **Random Forest classifier** using **Stratified K-Fold cross-validation** and **randomized hyperparameter search** to improve predictive performance on our Explore California product recommendation dataset.\n",
    "\n",
    "### What this function implements:\n",
    "\n",
    "- ‚úÖ **Train/validation split** using `train_test_split` with `stratify` to ensure class proportions are preserved\n",
    "- üî† **Label encoding** of the target column (`product_name`) using `LabelEncoder`\n",
    "- ‚öôÔ∏è **Random Forest classifier** inside a `Pipeline`, with support for class weighting (`class_weight='balanced'`) to handle class imbalance\n",
    "- üîÅ **Stratified K-Fold Cross-Validation** to get a more reliable estimate of model performance:\n",
    "  - The data is split into `k` folds (typically 5 but we'll reduce our default to 3 so it will run easier on our Google Colab environment)\n",
    "  - In each iteration, one fold is used as the validation set and the remaining `k-1` folds are used for training\n",
    "  - **Stratification** ensures that the class distribution remains similar across each fold\n",
    "  - This helps prevent biased evaluation, especially when dealing with imbalanced classes\n",
    "- üîç **Randomized Hyperparameter Search** via `RandomizedSearchCV`:\n",
    "  - Instead of exhaustively searching all combinations like Grid Search, it **randomly samples** a fixed number (`n_iter`) of combinations from a specified parameter distribution\n",
    "  - This makes it much faster while still exploring a broad set of configurations\n",
    "  - Hyperparameters being tuned include:\n",
    "    - `n_estimators` (number of trees)\n",
    "    - `max_depth` (maximum depth of trees)\n",
    "    - `min_samples_split` (min samples required to split an internal node)\n",
    "    - `min_samples_leaf` (min samples required to be a leaf node)\n",
    "\n",
    "---\n",
    "\n",
    "### Customization and control:\n",
    "\n",
    "You can pass your own values to control the training process:\n",
    "\n",
    "- `test_size`: Fraction of data to set aside for final validation\n",
    "- `n_splits`: Number of folds for cross-validation\n",
    "- `n_iter`: Number of random combinations to try in hyperparameter search\n",
    "- `scoring`: Evaluation metric used during tuning (e.g. `'f1_weighted'`)\n",
    "- `class_weight`: Strategy for handling class imbalance (`'balanced'` is a common default)\n",
    "- `param_dist`: A dictionary defining the range or distribution of hyperparameters to sample from\n",
    "\n",
    "This approach gives you a high quality, tunable modeling function ‚Äî ideal for building robust classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold_random_forest_model(\n",
    "    df,\n",
    "    target_col='product_name',\n",
    "    id_col='user_id',\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=5,\n",
    "    n_splits=3,\n",
    "    class_weight='balanced',\n",
    "    param_dist=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest model using Stratified K-Fold cross-validation and randomized search.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing features and a multi-class classification target.\n",
    "\n",
    "    target_col : str, default='product_name'\n",
    "        The name of the multi-class target column.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of the unique identifier column (excluded from training features).\n",
    "\n",
    "    drop_cols : list of str, optional\n",
    "        Additional columns to exclude from training. If None, defaults to [target_col, id_col].\n",
    "\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to use as the validation split.\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Seed used for reproducibility across train/test splits and random search.\n",
    "\n",
    "    scoring : str, default='f1_weighted'\n",
    "        Scoring metric to optimize during cross-validation (e.g., 'accuracy', 'precision_weighted').\n",
    "\n",
    "    n_iter : int, default=5\n",
    "        Number of parameter combinations to try in `RandomizedSearchCV`.\n",
    "\n",
    "    n_splits : int, default=3\n",
    "        Number of cross-validation folds for Stratified K-Fold.\n",
    "\n",
    "    class_weight : str, dict or None, default='balanced'\n",
    "        Class weight setting for the Random Forest.\n",
    "        - Use 'balanced' to correct for class imbalance.\n",
    "\n",
    "    param_dist : dict, optional\n",
    "        Dictionary specifying the hyperparameter search space.\n",
    "        If None, a default range will be used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the best estimator, label encoder, and train/validation splits.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import classification_report\n",
    "    from scipy.stats import randint\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Feature and target selection\n",
    "    # ---------------------------------------\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [target_col, id_col]\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # üî† Encode the target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # üîÄ Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, stratify=y_encoded, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # üì¶ Build pipeline with Random Forest\n",
    "    pipeline = Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(class_weight=class_weight, random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    # üîß Hyperparameter space\n",
    "    if param_dist is None:\n",
    "        param_dist = {\n",
    "            \"clf__n_estimators\": randint(50, 300),\n",
    "            \"clf__max_depth\": randint(5, 30),\n",
    "            \"clf__min_samples_split\": randint(2, 10),\n",
    "            \"clf__min_samples_leaf\": randint(1, 10)\n",
    "        }\n",
    "\n",
    "    # üîÅ Cross-validation strategy\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # üîç Randomized hyperparameter search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=kfold,\n",
    "        scoring=scoring,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # üöÇ Train the model\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # üèÜ Report best results\n",
    "    print(\"Best F1 Weighted Score:\", random_search.best_score_)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "    # üìä Evaluate on validation data\n",
    "    y_val_pred = random_search.predict(X_val)\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # üîÅ Return for downstream use\n",
    "    return {\n",
    "        \"model\": random_search.best_estimator_,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take ~1 minute to run!\n",
    "product_recommendations_random_forest = train_kfold_random_forest_model(product_recommendation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "  product_recommendations_random_forest,\n",
    "  \"Random Forest - Product Recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6d716",
   "metadata": {},
   "source": [
    "## 2.4 Model Performance Comparison: Random Forest vs Logistic Regression\n",
    "\n",
    "We trained two different models to predict user tour preferences:\n",
    "\n",
    "- A **Random Forest Classifier** with randomized hyperparameter tuning\n",
    "- A **Multiclass Logistic Regression** model with balanced class weights\n",
    "\n",
    "### ‚úÖ Summary of Results\n",
    "\n",
    "| Metric               | Random Forest | Logistic Regression |\n",
    "|----------------------|---------------|----------------------|\n",
    "| **Accuracy**         | 0.58          | 0.97                 |\n",
    "| **Macro F1-score**   | 0.52          | 0.93                 |\n",
    "| **Weighted F1-score**| 0.58          | 0.97                 |\n",
    "\n",
    "### üìä Key Observations\n",
    "\n",
    "- The **logistic regression model significantly outperforms** the random forest across all metrics, especially on:\n",
    "  - **Macro F1**: which treats all classes equally ‚Äî strong evidence that logistic regression generalizes better to minority classes.\n",
    "  - **Accuracy** and **Weighted F1**: which factor in class frequencies ‚Äî logistic regression still wins comfortably.\n",
    "\n",
    "- **Class-by-class performance**:\n",
    "  - Logistic regression shows **consistently high precision and recall** across all tour products.\n",
    "  - Random forest shows **weaker precision/recall**, particularly on low-support classes (e.g., Epic NorCal, Granite Dreams).\n",
    "\n",
    "### ü§î Why might a simpler logistic regression model perform better?\n",
    "\n",
    "1. **Feature space is binary and sparse**: Logistic regression often works very well with binary inputs and linear decision boundaries. Decision trees can overfit this kind of data, especially when sample sizes are small for some classes.\n",
    "\n",
    "2. **Class imbalance is handled explicitly** in logistic regression via `class_weight='balanced'`, while random forest may require more careful tuning to avoid bias toward frequent classes.\n",
    "\n",
    "3. **Regularization** in logistic regression (L2 penalty) helps prevent overfitting and improves generalization ‚Äî especially useful in high-dimensional spaces with many correlated features.\n",
    "\n",
    "4. **Random forest may be under-tuned**: Even with randomized search, it‚Äôs possible that the model didn‚Äôt find optimal hyperparameters for this task or needed more iterations/folds to stabilize.\n",
    "\n",
    "5. **Overfitting small classes**: Random forest can create overly complex trees that memorize training examples from smaller classes, which harms generalization.\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Conclusion**: In this case, the **simpler logistic regression model performs best** ‚Äî reinforcing the idea that more complex models don‚Äôt always lead to better results, especially when working with well-structured binary features and limited data per class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245eee9",
   "metadata": {},
   "source": [
    "## 2.5 Early Stopping with LightGBM\n",
    "\n",
    "In this section, we introduce a powerful training technique called **early stopping**, which helps prevent overfitting and speeds up training for gradient boosting models like LightGBM.\n",
    "\n",
    "### üß† What is Early Stopping?\n",
    "\n",
    "Early stopping is a **regularization technique** used during model training that monitors performance on a validation set. Training halts when the model's performance stops improving after a certain number of rounds ‚Äî preventing it from overfitting to the training data.\n",
    "\n",
    "- In our case, we monitor the **multi-class log loss** on the validation set.\n",
    "- If the model doesn't improve for `20 rounds`, training stops early and the best iteration is used for prediction.\n",
    "\n",
    "### ‚öôÔ∏è Adapting Pipelines for LightGBM\n",
    "\n",
    "Unlike scikit-learn‚Äôs standard classifiers, **LightGBM has its own training interface** that supports early stopping via callbacks. This requires a few changes:\n",
    "\n",
    "- We **don‚Äôt use a `Pipeline`** from scikit-learn ‚Äî instead, we call `.fit()` directly on the LightGBM model.\n",
    "- We pass the `eval_set`, `eval_metric`, and `early_stopping` **as arguments** to the `fit()` method.\n",
    "- The `LGBMClassifier` still accepts scikit-learn-compatible parameters like `class_weight`, `n_estimators`, and `learning_rate`.\n",
    "\n",
    "### üîç Key Parameters Used\n",
    "\n",
    "- `n_estimators=10000`: A large number of boosting rounds ‚Äî early stopping will decide the actual stopping point.\n",
    "- `early_stopping(stopping_rounds=20)`: Stops training if no improvement after 20 rounds.\n",
    "- `class_weight='balanced'`: Helps adjust for class imbalance.\n",
    "- `eval_metric='multi_logloss'`: Appropriate for multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Why use early stopping?**  \n",
    "It‚Äôs especially useful for boosting models like LightGBM and XGBoost, where too many iterations can lead to overfitting. Early stopping helps us **automatically find the optimal number of boosting rounds** ‚Äî improving both accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23253a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_early_stopping_model(\n",
    "    df,\n",
    "    target_col='product_name',\n",
    "    id_col='user_id',\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=10000,\n",
    "    class_weight='balanced',\n",
    "    early_stopping_rounds=20,\n",
    "    eval_metric='multi_logloss'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a LightGBM model with early stopping for multi-class classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing user attributes and a multi-class target label.\n",
    "\n",
    "    target_col : str, default='product_name'\n",
    "        The name of the multi-class target column.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of the identifier column to exclude from training.\n",
    "\n",
    "    drop_cols : list of str, optional\n",
    "        Additional columns to exclude from training. If None, defaults to [target_col, id_col].\n",
    "\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility in train/test splitting and model training.\n",
    "\n",
    "    learning_rate : float, default=0.1\n",
    "        Learning rate for boosting. Smaller values may yield better generalization but require more boosting rounds.\n",
    "\n",
    "    n_estimators : int, default=10000\n",
    "        Maximum number of boosting rounds. Training may stop earlier if early stopping criteria are met.\n",
    "\n",
    "    class_weight : str, dict, or None, default='balanced'\n",
    "        Weights associated with classes. 'balanced' automatically adjusts for class imbalance.\n",
    "\n",
    "    early_stopping_rounds : int, default=20\n",
    "        Number of rounds to wait for improvement before stopping training early.\n",
    "\n",
    "    eval_metric : str, default='multi_logloss'\n",
    "        Evaluation metric used for early stopping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the trained model, label encoder, and training/validation splits.\n",
    "    \"\"\"\n",
    "\n",
    "    from lightgbm import LGBMClassifier, early_stopping\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Feature and target selection\n",
    "    # ---------------------------------------\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [target_col, id_col]\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # üî† Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # üîÄ Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, stratify=y_encoded, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # üì¶ Configure LightGBM classifier\n",
    "    model = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=len(label_encoder.classes_),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        class_weight=class_weight,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ‚ö° Train with early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=eval_metric,\n",
    "        callbacks=[early_stopping(stopping_rounds=early_stopping_rounds)]\n",
    "    )\n",
    "\n",
    "    # üìä Evaluate on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # üîÅ Return model and metadata for downstream use\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae251a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take ~1 minute to run!\n",
    "product_recommendations_lightgbm = train_lightgbm_early_stopping_model(product_recommendation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(product_recommendations_lightgbm, \"LightGBM - Product Recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466aa3f6",
   "metadata": {},
   "source": [
    "## 2.5 LightGBM with Optuna (Bayesian Optimization)\n",
    "\n",
    "In this section, we‚Äôll train a **LightGBM model** using **Optuna**, a Python library for **Bayesian hyperparameter optimization**. Compared to traditional grid or random search, this approach is more efficient and intelligent ‚Äî especially when tuning many hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What is Bayesian Optimization?\n",
    "\n",
    "Think of hyperparameter tuning like searching for the best restaurant in a new city:\n",
    "\n",
    "- **Grid search** is like trying every single restaurant on a giant map ‚Äî time-consuming and inefficient.\n",
    "- **Random search** is like picking restaurants at random and hoping one is great.\n",
    "- **Bayesian optimization** is like asking locals for recommendations, trying the best one, then adjusting your search based on what you liked ‚Äî getting smarter with each try.\n",
    "\n",
    "Optuna implements this smarter strategy by learning from past trials and focusing on the most promising combinations.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ What does this function do?\n",
    "\n",
    "1. **Defines an objective function**  \n",
    "   - We wrap the model training and evaluation into a function that returns a **macro-averaged F1 score**, which is a good fit for multi-class tasks.\n",
    "   - ‚úÖ Inside this objective function, we use **early stopping** during model training to avoid overfitting and reduce unnecessary computation.\n",
    "\n",
    "2. **Launches an Optuna study**  \n",
    "   - The study runs 10 trials, each one suggesting a different set of LightGBM hyperparameters (like `learning_rate`, `num_leaves`, `lambda_l1`, etc.).\n",
    "   - After each trial, Optuna updates its internal model to focus on better-performing regions of the hyperparameter space.\n",
    "\n",
    "3. **Trains the final model**  \n",
    "   - After optimization, we retrain a LightGBM model using the best hyperparameters discovered by Optuna.\n",
    "\n",
    "4. **Evaluates model performance**  \n",
    "   - We report the **accuracy**, **macro F1 score**, and a **full classification report** on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Why this matters\n",
    "\n",
    "- **Faster convergence**: Bayesian optimization often finds strong models with fewer trials.\n",
    "- **Efficient training**: By using **early stopping in each trial**, we ensure that every model is trained as well as possible ‚Äî without wasting time on poor configurations.\n",
    "- **Better performance**: We're directly optimizing for the metric that matters most.\n",
    "- **Powerful for complex models**: This approach shines when tuning multiple parameters in high-capacity models like LightGBM.\n",
    "\n",
    "If you've tried grid or random search, this method is the natural next step ‚Äî combining **smart exploration** with **practical safeguards like early stopping** for faster, better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e38a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_optuna_model(\n",
    "    df,\n",
    "    target_col='product_name',\n",
    "    id_col='user_id',\n",
    "    drop_cols=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    n_trials=10,\n",
    "    scoring_metric='macro'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a LightGBM model using Optuna for Bayesian hyperparameter optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing user features and a multi-class classification target.\n",
    "\n",
    "    target_col : str, default='product_name'\n",
    "        The name of the multi-class target column.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        The name of the identifier column to exclude from training.\n",
    "\n",
    "    drop_cols : list of str, optional\n",
    "        Additional columns to exclude from training. If None, defaults to [target_col, id_col].\n",
    "\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility in data splitting and model training.\n",
    "\n",
    "    n_trials : int, default=10\n",
    "        Number of trials for Optuna to explore in the hyperparameter search space.\n",
    "\n",
    "    scoring_metric : str, default='macro'\n",
    "        Type of F1 averaging to use when evaluating performance ('macro', 'weighted', etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the best trained model, label encoder, and training/validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    from lightgbm import LGBMClassifier, early_stopping\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "    import optuna\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Feature and target selection\n",
    "    # ---------------------------------------\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [target_col, id_col]\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # üî† Encode the target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # üîÄ Split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, stratify=y_encoded, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Define Optuna objective function\n",
    "    # ---------------------------------------\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"num_class\": len(label_encoder.classes_),\n",
    "            \"metric\": \"multi_logloss\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"random_state\": random_state,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 100),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        return f1_score(y_val, preds, average=scoring_metric)\n",
    "\n",
    "    # üîç Run Optuna study\n",
    "    sampler = optuna.samplers.TPESampler(seed=10)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # üèÜ Retrieve best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    best_params[\"objective\"] = \"multiclass\"\n",
    "    best_params[\"num_class\"] = len(label_encoder.classes_)\n",
    "\n",
    "    # üöÇ Train final model with best parameters\n",
    "    final_model = LGBMClassifier(**best_params)\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # üìä Evaluate final model\n",
    "    y_val_pred = final_model.predict(X_val)\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "    print(f\"F1 Score ({scoring_metric}):\", f1_score(y_val, y_val_pred, average=scoring_metric))\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # üîÅ Return model and training artifacts\n",
    "    return {\n",
    "        \"model\": final_model,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take ~2 minutes to run!\n",
    "product_recommendations_bayesian_optimization_lightgbm = train_lightgbm_optuna_model(product_recommendation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "  product_recommendations_bayesian_optimization_lightgbm,\n",
    "  \"Optuna LightGBM - Product Recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccc081",
   "metadata": {},
   "source": [
    "## 2.6 Final Model Comparison\n",
    "\n",
    "We‚Äôve trained and evaluated four models for our product recommendation task:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest**\n",
    "3. **LightGBM with Early Stopping**\n",
    "4. **LightGBM with Optuna Bayesian Optimization**\n",
    "\n",
    "Before we dive into detailed insights, here‚Äôs a high-level summary of the performance metrics:\n",
    "\n",
    "| Model                           | Accuracy | Macro F1 Score | Weighted F1 Score |\n",
    "|--------------------------------|----------|----------------|-------------------|\n",
    "| **Logistic Regression**        | 0.97     | 0.93           | 0.97              |\n",
    "| **Random Forest**              | 0.59     | 0.52           | 0.59              |\n",
    "| **LightGBM (Early Stopping)**  | 0.80     | 0.64           | 0.79              |\n",
    "| **LightGBM (Optuna Optimized)**| 0.70     | 0.56           | 0.69              |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. Logistic Regression\n",
    "\n",
    "- **Accuracy**: 97%  \n",
    "- **Macro F1 Score**: 0.93  \n",
    "- **Weighted F1 Score**: 0.97  \n",
    "\n",
    "**Observations:**\n",
    "- Strong precision and recall across all classes.\n",
    "- Consistently high performance even for minority classes (e.g., *Epic NorCal Expedition*, *Giant Trees*).\n",
    "- Nearly perfect classification for popular tours.\n",
    "\n",
    "‚úîÔ∏è **Best overall performance.**  \n",
    "üìå *Simple, robust, and surprisingly well-suited to the dataset.*\n",
    "\n",
    "---\n",
    "\n",
    "### üå≤ 2. Random Forest\n",
    "\n",
    "- **Accuracy**: 59%  \n",
    "- **Macro F1 Score**: 0.52  \n",
    "- **Weighted F1 Score**: 0.59  \n",
    "\n",
    "**Observations:**\n",
    "- Struggles with precision and recall across almost all classes.\n",
    "- High variance ‚Äî some classes overfit, others underperform.\n",
    "- Appears unable to capture complex feature interactions meaningfully in this setting.\n",
    "\n",
    "‚ö†Ô∏è **Worst overall performance.**  \n",
    "üìå *Could indicate overfitting or weak signal extraction from binary features.*\n",
    "\n",
    "---\n",
    "\n",
    "### üåü 3. LightGBM with Early Stopping\n",
    "\n",
    "- **Accuracy**: 80%  \n",
    "- **Macro F1 Score**: 0.64  \n",
    "- **Weighted F1 Score**: 0.79  \n",
    "\n",
    "**Observations:**\n",
    "- Stronger performance than Random Forest.\n",
    "- Very high precision but **poor recall** for minority classes (e.g., *Epic NorCal*, *Golden State Wonders*).\n",
    "- May be over-prioritizing dominant classes.\n",
    "\n",
    "üü° **Moderate performance, with high class imbalance sensitivity.**  \n",
    "üìå *Early stopping helps prevent overfitting but may not be enough without class-specific tuning.*\n",
    "\n",
    "---\n",
    "\n",
    "### üîß 4. LightGBM + Optuna (Bayesian Optimization)\n",
    "\n",
    "- **Accuracy**: 70%  \n",
    "- **Macro F1 Score**: 0.56  \n",
    "- **Weighted F1 Score**: 0.69  \n",
    "\n",
    "**Observations:**\n",
    "- Better recall for minority classes compared to early stopping model.\n",
    "- More balanced precision‚Äìrecall tradeoff.\n",
    "- Still trails behind Logistic Regression.\n",
    "- Might still be slightly underfit and could do with more than 10 study trials.\n",
    "\n",
    "üü† **Improvement via Bayesian tuning ‚Äî but still not best in class.**  \n",
    "üìå *Shows value of tuning, but LightGBM still seems less well-suited to this dataset.*\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î Why Did **Logistic Regression** Win?\n",
    "\n",
    "Even though it‚Äôs a linear model, Logistic Regression performed **significantly better** than the more complex models. Here‚Äôs why:\n",
    "\n",
    "| Reason | Explanation |\n",
    "|--------|-------------|\n",
    "| ‚úÖ **Binary Inputs** | The dataset uses **binary features**. Logistic regression handles these well without needing complex tree splits. |\n",
    "| ‚úÖ **Low Feature Interaction** | If interactions between features are limited, linear models may generalize better than over-parameterized tree models. |\n",
    "| ‚úÖ **Well-Calibrated Class Balance** | Logistic regression with stratified train/test splits can handle class imbalance reasonably with fewer tuning knobs. |\n",
    "| ‚ö†Ô∏è **Tree Models Overfit** | Random Forest and LightGBM can overfit if the signal is weak or if hyperparameters aren't tuned precisely ‚Äî especially with small class sizes. |\n",
    "| ‚úÖ **Class Separation is Linearly Separable** | The features likely provide **enough separation** in a linear decision space ‚Äî no complex boundaries needed. |\n",
    "\n",
    "---\n",
    "\n",
    "### üîö Final Thoughts\n",
    "\n",
    "- **Start simple.** Always benchmark with logistic regression ‚Äî you might be surprised.\n",
    "- **Don‚Äôt blindly trust complexity.** Tree-based models offer flexibility, but may **underperform** if the data structure doesn‚Äôt demand it.\n",
    "- **Tune with care.** Even Optuna can‚Äôt save a model that doesn‚Äôt fit the problem‚Äôs shape.\n",
    "\n",
    "Let‚Äôs now take these insights and think about how we could further improve our tree-based models:\n",
    "- Try **feature engineering** or interaction terms.\n",
    "- Use **calibrated class weights** or focal loss for imbalanced classes.\n",
    "- Explore **embedding** sparse features if the space becomes large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefe0d5",
   "metadata": {},
   "source": [
    "# 3. Binary Classification\n",
    "\n",
    "In this section, we‚Äôll shift our focus from multi-class product recommendations to a **binary classification task** ‚Äî predicting whether a user will purchase *any* product within a defined time period. In our case, it's within 30 days from the provided `label_date`.\n",
    "\n",
    "We'll be using our binary classification dataset `purchase_prediction_df`, which includes both user attributes and pre-assigned training, validation, and test periods.\n",
    "\n",
    "Here's what we'll be covering in this section:\n",
    "\n",
    "1. **Temporal Data Splitting**  \n",
    "   We'll use the provided `train`, `val`, and `test` flags in the dataset ‚Äî simulating how we typically train on past data and validate on future outcomes.\n",
    "\n",
    "2. **Robust Data Preprocessing**  \n",
    "   We'll adapt our `scikit-learn` pipeline to:\n",
    "   - Handle missing values using a simple imputation strategy  \n",
    "   - Prepare binary target labels for classification\n",
    "\n",
    "3. **Model 1: Logistic Regression**  \n",
    "   We‚Äôll fit a simple, interpretable baseline model and evaluate how well it separates buyers from non-buyers.\n",
    "\n",
    "4. **Model 2: LightGBM with Optuna Optimization**  \n",
    "   Using **Bayesian optimization**, we‚Äôll tune a more powerful model to see if we can improve upon the baseline.\n",
    "\n",
    "5. **Model Evaluation**  \n",
    "   Instead of accuracy, we‚Äôll use:\n",
    "   - **AUC (Area Under the ROC Curve)**  \n",
    "   - **ROC Curves** to visualize true positive vs. false positive tradeoffs\n",
    "\n",
    "6. **Model Explainability with SHAP and LIME**  \n",
    "   Predictive performance is important ‚Äî but equally critical is being able to understand and explain **why** your model made a certain decision.\n",
    "\n",
    "   In this final step, we'll:\n",
    "   - Use **SHAP (SHapley Additive exPlanations)** to get global and local feature importance scores based on game theory.\n",
    "   - Use **LIME (Local Interpretable Model-Agnostic Explanations)** to explain individual predictions by fitting simple models locally.\n",
    "\n",
    "   These techniques help us:\n",
    "   - Build trust with stakeholders  \n",
    "   - Detect spurious correlations  \n",
    "   - Comply with model transparency requirements in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c0513",
   "metadata": {},
   "source": [
    "## 3.1 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe41651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_logistic_regression_model_with_temporal_split(\n",
    "    df,\n",
    "    target_col='label',\n",
    "    positive_class=1,\n",
    "    id_col='user_id',\n",
    "    drop_cols=['label_date'],\n",
    "    period_col='period',\n",
    "    class_weight='balanced',\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a binary logistic regression model using temporal splits.\n",
    "    Plots ROC and Precision-Recall curves for train, validation, and test sets (if available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Trained model, predictions, probabilities, label encoder, and evaluation data splits.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import (\n",
    "        classification_report, roc_auc_score, roc_curve,\n",
    "        precision_recall_curve\n",
    "    )\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Preprocessing\n",
    "    # ---------------------------------------\n",
    "    df = df.fillna(0)\n",
    "    drop_cols = drop_cols or []\n",
    "    drop_cols = list(set(drop_cols + [target_col, id_col, period_col]))\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y_raw = df[target_col]\n",
    "\n",
    "    if y_raw.nunique() > 2:\n",
    "        raise ValueError(\"Target column must be binary.\")\n",
    "\n",
    "    if y_raw.dtype == 'object' or not set(y_raw.unique()).issubset({0, 1}):\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y_raw == positive_class)\n",
    "    else:\n",
    "        label_encoder = None\n",
    "        y_encoded = (y_raw == positive_class).astype(int)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Temporal Split\n",
    "    # ---------------------------------------\n",
    "    period = df[period_col]\n",
    "    train_mask = period == \"train\"\n",
    "    val_mask = period == \"validation\"\n",
    "    test_mask = period == \"test\"\n",
    "\n",
    "    X_train, y_train = X[train_mask], y_encoded[train_mask]\n",
    "    X_val, y_val = X[val_mask], y_encoded[val_mask]\n",
    "    X_test, y_test = X[test_mask], y_encoded[test_mask] if test_mask.any() else (None, None)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Train Model\n",
    "    # ---------------------------------------\n",
    "    model = LogisticRegression(\n",
    "        class_weight=class_weight,\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Evaluate on Train Set\n",
    "    # ---------------------------------------\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "    precision_train, recall_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "    auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "    print(\"üìä Train Classification Report:\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Evaluate on Validation Set\n",
    "    # ---------------------------------------\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
    "    precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "    auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "    print(\"\\nüìä Validation Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Evaluate on Test Set (Optional)\n",
    "    # ---------------------------------------\n",
    "    if test_mask.any():\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "        precision_test, recall_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "        auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "        print(\"\\nüìä Test Classification Report:\")\n",
    "        print(classification_report(y_test, y_test_pred))\n",
    "    else:\n",
    "        y_test_proba, fpr_test, tpr_test = None, None, None\n",
    "        precision_test, recall_test, auc_test = None, None, None\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Plot ROC + Precision-Recall Curves\n",
    "    # ---------------------------------------\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "    # ROC Curves\n",
    "    axes[0, 0].plot(fpr_train, tpr_train, label=f\"AUC = {auc_train:.2f}\", lw=2)\n",
    "    axes[0, 0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    axes[0, 0].set_title(\"ROC Curve (Train)\")\n",
    "    axes[0, 0].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0, 0].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].plot(fpr_val, tpr_val, label=f\"AUC = {auc_val:.2f}\", lw=2)\n",
    "    axes[0, 1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    axes[0, 1].set_title(\"ROC Curve (Validation)\")\n",
    "    axes[0, 1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0, 1].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    if fpr_test is not None:\n",
    "        axes[0, 2].plot(fpr_test, tpr_test, label=f\"AUC = {auc_test:.2f}\", lw=2)\n",
    "        axes[0, 2].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "        axes[0, 2].set_title(\"ROC Curve (Test)\")\n",
    "        axes[0, 2].set_xlabel(\"False Positive Rate\")\n",
    "        axes[0, 2].set_ylabel(\"True Positive Rate\")\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "\n",
    "    # Precision-Recall Curves\n",
    "    axes[1, 0].plot(recall_train, precision_train, label=\"Train\", lw=2)\n",
    "    axes[1, 0].set_title(\"Precision-Recall (Train)\")\n",
    "    axes[1, 0].set_xlabel(\"Recall\")\n",
    "    axes[1, 0].set_ylabel(\"Precision\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    axes[1, 1].plot(recall_val, precision_val, label=\"Validation\", lw=2)\n",
    "    axes[1, 1].set_title(\"Precision-Recall (Validation)\")\n",
    "    axes[1, 1].set_xlabel(\"Recall\")\n",
    "    axes[1, 1].set_ylabel(\"Precision\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    if precision_test is not None:\n",
    "        axes[1, 2].plot(recall_test, precision_test, label=\"Test\", lw=2)\n",
    "        axes[1, 2].set_title(\"Precision-Recall (Test)\")\n",
    "        axes[1, 2].set_xlabel(\"Recall\")\n",
    "        axes[1, 2].set_ylabel(\"Precision\")\n",
    "        axes[1, 2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Return Training Artifacts\n",
    "    # ---------------------------------------\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test if test_mask.any() else None,\n",
    "        \"y_test\": y_test if test_mask.any() else None,\n",
    "        \"y_proba_train\": y_train_proba,\n",
    "        \"y_proba_val\": y_val_proba,\n",
    "        \"y_proba_test\": y_test_proba if test_mask.any() else None,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist(),\n",
    "        \"curve_plot\": fig\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c20171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should take 30 seconds to run!\n",
    "purchase_prediction_logistic_regression = train_binary_logistic_regression_model_with_temporal_split(\n",
    "  purchase_prediction_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574036b",
   "metadata": {},
   "source": [
    "## 3.2 LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_optuna_binary_with_temporal_split(\n",
    "    df,\n",
    "    target_col='label',\n",
    "    positive_class=1,\n",
    "    id_col='user_id',\n",
    "    drop_cols=['label_date'],\n",
    "    period_col='period',\n",
    "    n_trials=20,\n",
    "    scoring_metric='binary',\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a binary LightGBM model using Optuna for hyperparameter tuning with temporal train/val/test splits.\n",
    "    Includes train, validation, and test ROC and Precision-Recall curves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with features, binary target, and 'period' column.\n",
    "\n",
    "    target_col : str, default='label'\n",
    "        Binary classification target column.\n",
    "\n",
    "    positive_class : int or str, default=1\n",
    "        Value treated as positive class.\n",
    "\n",
    "    id_col : str, default='user_id'\n",
    "        Identifier column to exclude from training.\n",
    "\n",
    "    drop_cols : list of str, optional\n",
    "        Additional columns to exclude (e.g., ['label_date']).\n",
    "\n",
    "    period_col : str, default='period'\n",
    "        Column containing 'train', 'validation', and 'test' labels.\n",
    "\n",
    "    n_trials : int, default=20\n",
    "        Number of Optuna trials.\n",
    "\n",
    "    scoring_metric : str, default='binary'\n",
    "        Averaging method for F1 ('binary', 'macro', etc.).\n",
    "\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with model, splits, probabilities, and encoder.\n",
    "    \"\"\"\n",
    "    import optuna\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lightgbm import LGBMClassifier, early_stopping\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import (\n",
    "        classification_report, f1_score,\n",
    "        roc_auc_score, roc_curve, precision_recall_curve\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Preprocessing and Target Encoding\n",
    "    # ---------------------------------------\n",
    "    df = df.fillna(0)\n",
    "    drop_cols = drop_cols or []\n",
    "    drop_cols = list(set(drop_cols + [target_col, id_col, period_col]))\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y_raw = df[target_col]\n",
    "\n",
    "    if y_raw.nunique() > 2:\n",
    "        raise ValueError(\"Target column must be binary.\")\n",
    "\n",
    "    if y_raw.dtype == 'object' or not set(y_raw.unique()).issubset({0, 1}):\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y_raw == positive_class)\n",
    "    else:\n",
    "        label_encoder = None\n",
    "        y_encoded = (y_raw == positive_class).astype(int)\n",
    "\n",
    "    period = df[period_col]\n",
    "    train_mask = period == \"train\"\n",
    "    val_mask = period == \"validation\"\n",
    "    test_mask = period == \"test\"\n",
    "\n",
    "    X_train, y_train = X[train_mask], y_encoded[train_mask]\n",
    "    X_val, y_val = X[val_mask], y_encoded[val_mask]\n",
    "    X_test, y_test = X[test_mask], y_encoded[test_mask] if test_mask.any() else (None, None)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Optuna Hyperparameter Tuning\n",
    "    # ---------------------------------------\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": \"binary_logloss\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"random_state\": random_state,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 100),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        return f1_score(y_val, preds, average=scoring_metric)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Final Model Training\n",
    "    # ---------------------------------------\n",
    "    best_params = study.best_trial.params\n",
    "    best_params[\"objective\"] = \"binary\"\n",
    "    final_model = LGBMClassifier(**best_params)\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Evaluation on Train/Val/Test Sets\n",
    "    # ---------------------------------------\n",
    "    def evaluate_split(X, y, name=\"Set\"):\n",
    "        y_pred = final_model.predict(X)\n",
    "        y_proba = final_model.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "        auc = roc_auc_score(y, y_proba)\n",
    "        print(f\"\\nüìä {name} Classification Report:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "        return {\n",
    "            \"y_pred\": y_pred,\n",
    "            \"y_proba\": y_proba,\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"auc\": auc\n",
    "        }\n",
    "\n",
    "    eval_train = evaluate_split(X_train, y_train, name=\"Train\")\n",
    "    eval_val = evaluate_split(X_val, y_val, name=\"Validation\")\n",
    "    eval_test = evaluate_split(X_test, y_test, name=\"Test\") if X_test is not None else None\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Plot Curves for Train / Val / Test\n",
    "    # ---------------------------------------\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "    # ROC Curves\n",
    "    axes[0, 0].plot(eval_train[\"fpr\"], eval_train[\"tpr\"], label=f\"AUC = {eval_train['auc']:.2f}\")\n",
    "    axes[0, 0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    axes[0, 0].set_title(\"ROC Curve (Train)\")\n",
    "    axes[0, 0].set_xlabel(\"FPR\")\n",
    "    axes[0, 0].set_ylabel(\"TPR\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].plot(eval_val[\"fpr\"], eval_val[\"tpr\"], label=f\"AUC = {eval_val['auc']:.2f}\")\n",
    "    axes[0, 1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    axes[0, 1].set_title(\"ROC Curve (Validation)\")\n",
    "    axes[0, 1].set_xlabel(\"FPR\")\n",
    "    axes[0, 1].set_ylabel(\"TPR\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    if eval_test:\n",
    "        axes[0, 2].plot(eval_test[\"fpr\"], eval_test[\"tpr\"], label=f\"AUC = {eval_test['auc']:.2f}\")\n",
    "        axes[0, 2].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "        axes[0, 2].set_title(\"ROC Curve (Test)\")\n",
    "        axes[0, 2].set_xlabel(\"FPR\")\n",
    "        axes[0, 2].set_ylabel(\"TPR\")\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "\n",
    "    # Precision-Recall Curves\n",
    "    axes[1, 0].plot(eval_train[\"recall\"], eval_train[\"precision\"])\n",
    "    axes[1, 0].set_title(\"Precision-Recall (Train)\")\n",
    "    axes[1, 0].set_xlabel(\"Recall\")\n",
    "    axes[1, 0].set_ylabel(\"Precision\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    axes[1, 1].plot(eval_val[\"recall\"], eval_val[\"precision\"])\n",
    "    axes[1, 1].set_title(\"Precision-Recall (Validation)\")\n",
    "    axes[1, 1].set_xlabel(\"Recall\")\n",
    "    axes[1, 1].set_ylabel(\"Precision\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    if eval_test:\n",
    "        axes[1, 2].plot(eval_test[\"recall\"], eval_test[\"precision\"])\n",
    "        axes[1, 2].set_title(\"Precision-Recall (Test)\")\n",
    "        axes[1, 2].set_xlabel(\"Recall\")\n",
    "        axes[1, 2].set_ylabel(\"Precision\")\n",
    "        axes[1, 2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Return Training Artifacts\n",
    "    # ---------------------------------------\n",
    "    return {\n",
    "        \"model\": final_model,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_proba_train\": eval_train[\"y_proba\"],\n",
    "        \"y_proba_val\": eval_val[\"y_proba\"],\n",
    "        \"y_proba_test\": eval_test[\"y_proba\"] if eval_test else None,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"attribute_names\": X_train.columns.tolist(),\n",
    "        \"curve_plot\": fig\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2386fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should take ~2 minutes to run!\n",
    "purchase_prediction_optuna_lightgbm = train_lightgbm_optuna_binary_with_temporal_split(\n",
    "  purchase_prediction_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89887a",
   "metadata": {},
   "source": [
    "## 3.3 Model Comparison: Logistic Regression vs LightGBM\n",
    "\n",
    "We evaluate two binary classifiers ‚Äî **Logistic Regression** and **LightGBM** ‚Äî on the purchase prediction task across the training, validation, and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Classification Report Summary\n",
    "\n",
    "| Metric         | Model               | Train Set | Validation Set | Test Set |\n",
    "|----------------|---------------------|-----------|----------------|----------|\n",
    "| **Accuracy**   | Logistic Regression | 63%       | 64%            | 64%      |\n",
    "|                | LightGBM            | 79%       | 91%            | 84%      |\n",
    "| **Recall**     | Logistic Regression | 59%       | 57%            | 61%      |\n",
    "|                | LightGBM            | 26%       | 11%            | 13%      |\n",
    "| **Precision**  | Logistic Regression | 38%       | 8%             | 22%      |\n",
    "|                | LightGBM            | 90%       | 12%            | 32%      |\n",
    "| **F1 Score**   | Logistic Regression | 46%       | 14%            | 33%      |\n",
    "|                | LightGBM            | 40%       | 11%            | 18%      |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Takeaways\n",
    "\n",
    "#### 1. **Overall Accuracy**\n",
    "- LightGBM achieves significantly higher accuracy across all sets, especially validation (91%) and test (84%).\n",
    "- However, high accuracy is **driven by dominance of the majority class (non-purchasers)**.\n",
    "\n",
    "#### 2. **Recall for Class 1 (Purchasers)**\n",
    "- Logistic Regression consistently outperforms LightGBM on recall: it identifies more actual purchasers.\n",
    "- LightGBM misses most purchase cases ‚Äî recall drops to 11‚Äì13% on validation/test.\n",
    "\n",
    "> üß† If recall is critical (e.g. targeting all likely purchasers), Logistic Regression is more suitable.\n",
    "\n",
    "#### 3. **Precision for Class 1 (Purchasers)**\n",
    "- Logistic Regression shows low precision (as low as 8% on validation), meaning many false positives.\n",
    "- LightGBM maintains better precision (up to 32%) by being selective ‚Äî but at the cost of recall.\n",
    "\n",
    "> üéØ If avoiding false positives is the goal (e.g. expensive follow-up), LightGBM may be preferred.\n",
    "\n",
    "#### 4. **F1 Score for Class 1**\n",
    "- Logistic Regression has a stronger F1 score on the test set (0.33 vs 0.18), showing more balanced performance.\n",
    "- LightGBM‚Äôs sharp recall drop outweighs its precision advantage.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpretation\n",
    "\n",
    "- **Logistic Regression** is more balanced overall and better at surfacing positive cases.\n",
    "- **LightGBM** is overconfident in class 0 predictions and may require:\n",
    "  - Threshold tuning\n",
    "  - Class reweighting or oversampling\n",
    "  - Custom evaluation metrics (e.g., F1-score or recall for class 1)\n",
    "\n",
    "---\n",
    "\n",
    "### üî≠ Next Steps\n",
    "\n",
    "- Analyze the ROC and Precision-Recall curves to find optimal thresholds.\n",
    "- Consider **adjusting class weights** or **sampling strategies** to address imbalance.\n",
    "- Choose a model based on **your business priority**:\n",
    "  - High recall ‚Üí Logistic Regression\n",
    "  - High precision or accuracy ‚Üí LightGBM (with tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e435d",
   "metadata": {},
   "source": [
    "## 3.4 Curve Comparisons\n",
    "\n",
    "Below we compare the ROC and Precision-Recall curves of both models across **train**, **validation**, and **test** sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0db305",
   "metadata": {},
   "source": [
    "### 3.4.1 Logistic Regression Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_logistic_regression[\"curve_plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5f335",
   "metadata": {},
   "source": [
    "### 3.4.2 LightGBM Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_optuna_lightgbm[\"curve_plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5541230",
   "metadata": {},
   "source": [
    "### 3.4.3 ROC Curves (Discrimination Power)\n",
    "\n",
    "| Metric         | Logistic Regression | LightGBM      |\n",
    "|----------------|---------------------|---------------|\n",
    "| **AUC (Train)**     | 0.66                | **0.86**        |\n",
    "| **AUC (Validation)**| 0.66                | 0.65            |\n",
    "| **AUC (Test)**      | 0.66                | 0.65            |\n",
    "\n",
    "- **LightGBM shows stronger discrimination** on the training set (AUC = 0.86), but this does **not translate to validation/test**.\n",
    "- Both models generalize similarly on unseen data (AUC ‚âà 0.65‚Äì0.66), suggesting:\n",
    "  - LightGBM may be overfitting slightly.\n",
    "  - Logistic Regression generalizes more consistently.\n",
    "\n",
    "> üß† Interpretation: Logistic Regression is less powerful, but stable. LightGBM has stronger learning capacity, but risks overconfidence without tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4.4 Precision-Recall Curves\n",
    "\n",
    "- **Train PR Curve**: LightGBM dominates with **high precision across the recall spectrum**, suggesting great ranking capability in-sample.\n",
    "- **Validation & Test PR Curves**:\n",
    "  - Both models show **low precision** across the board, especially at moderate recall.\n",
    "  - Precision drops quickly beyond the top-ranked predictions.\n",
    "\n",
    "| Behavior                          | Logistic Regression       | LightGBM                     |\n",
    "|----------------------------------|----------------------------|------------------------------|\n",
    "| üìâ Precision at High Recall      | Drops sharply              | Drops sharply                |\n",
    "| üéØ Precision at Low Recall       | Lower but smoother         | Higher, then steep drop-off  |\n",
    "| üß™ Overfitting Signal (PR shape) | More stable across splits  | Train ‚â´ Validation/Test gap  |\n",
    "\n",
    "> üìå Precision-recall curves confirm that **LightGBM is more confident**, but it doesn't rank unseen positives well without tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4.5 Overall Model Summary\n",
    "\n",
    "| Aspect               | Logistic Regression                     | LightGBM                            |\n",
    "|----------------------|------------------------------------------|-------------------------------------|\n",
    "| **Generalization**   | Consistent AUC across splits             | Slight train-overfit on AUC         |\n",
    "| **Recall for Class 1**| Higher out-of-the-box                   | Very low unless threshold is tuned  |\n",
    "| **Precision**        | Lower overall, smoother                 | High at top, drops fast             |\n",
    "| **Recommendation**   | Balanced recall model                   | Calibrate threshold for precision-first use cases |\n",
    "\n",
    "Possible further next steps might include the following:\n",
    "\n",
    "- üìâ **Threshold Tuning**: Consider adjusting LightGBM‚Äôs decision threshold using PR curve inflection points.\n",
    "- ‚öñÔ∏è **Custom Objective**: Use F1-score or recall@k if positive class coverage matters.\n",
    "- ‚öôÔ∏è **Resampling or Class Weights**: Especially for LightGBM, to correct its strong bias toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca88a48",
   "metadata": {},
   "source": [
    "# 4. Model Explanations with SHAP and LIME\n",
    "\n",
    "Now that we‚Äôve trained our binary classification model, it‚Äôs time to move beyond accuracy and performance metrics.\n",
    "\n",
    "While metrics like F1-score and ROC-AUC are helpful to **quantify** performance, they don‚Äôt tell us **why** the model is making a particular prediction ‚Äî especially on edge cases or misclassifications.\n",
    "\n",
    "In many real-world applications, especially in regulated or sensitive domains, we must ensure that our models are:\n",
    "- **Interpretable**: Can a human understand the key factors behind a prediction?\n",
    "- **Transparent**: Can we explain both correct and incorrect outcomes?\n",
    "- **Accountable**: Can we defend model behavior if it leads to decisions that affect real users?\n",
    "\n",
    "To address this, we‚Äôll use the function `generate_binary_classification_model_explanations`, which combines two powerful tools:\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)**\n",
    "- Provides a **global view** of which features are driving model predictions on average.\n",
    "- Also supports **local explanations** by showing how feature values contribute to a single prediction.\n",
    "\n",
    "**LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "- Provides **case-by-case analysis** by fitting interpretable local models.\n",
    "- In this function, we use LIME to explain:\n",
    "  - ‚úÖ True Positives\n",
    "  - ‚úÖ True Negatives\n",
    "  - ‚ùå False Positives\n",
    "  - ‚ùå False Negatives\n",
    "\n",
    "This categorization helps us **scrutinize individual decisions**, identify systematic issues, and build trust in the model‚Äôs reasoning process.\n",
    "\n",
    "> ‚ö†Ô∏è Remember: Building an accurate model is great ‚Äî but not enough. Responsible ML practice requires understanding what‚Äôs happening under the hood, especially when deploying into production environments with fairness, compliance, or user trust at stake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_classification_model_explanations(model_outputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generate SHAP and LIME explanations for a binary classification model.\n",
    "    LIME samples include examples from all 4 prediction categories:\n",
    "    - True Positive, True Negative, False Positive, False Negative\n",
    "\n",
    "    Parameters:\n",
    "    - model_outputs: Dictionary with model, data splits, labels, and feature names\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with SHAP and LIME plotting elements (matplotlib Figure objects)\n",
    "    \"\"\"\n",
    "    # --- Local imports ---\n",
    "    import shap\n",
    "    import lime.lime_tabular\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # --- Unpack input dictionary ---\n",
    "    model = model_outputs[\"model\"]\n",
    "    X_train = model_outputs[\"X_train\"]\n",
    "    X_val = model_outputs[\"X_val\"]\n",
    "    y_val = model_outputs[\"y_val\"]\n",
    "    attribute_names = model_outputs.get(\"attribute_names\", X_train.columns.tolist())\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Get class names\n",
    "    if \"label_encoder\" in model_outputs and hasattr(model_outputs[\"label_encoder\"], \"inverse_transform\"):\n",
    "        class_names = list(model_outputs[\"label_encoder\"].inverse_transform([0, 1]))\n",
    "    else:\n",
    "        class_names = [\"Class 0\", \"Class 1\"]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # --- SHAP Explanations ---\n",
    "    print(\"üîç Generating SHAP explanations...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "    fig_bar = plt.figure()\n",
    "    shap.summary_plot(shap_values, X_val, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"SHAP Feature Importance (Bar)\")\n",
    "    results[\"shap_summary_bar_fig\"] = fig_bar\n",
    "\n",
    "    fig_summary = plt.figure()\n",
    "    shap.summary_plot(shap_values, X_val, show=False)\n",
    "    plt.title(\"SHAP Summary Plot\")\n",
    "    results[\"shap_summary_fig\"] = fig_summary\n",
    "\n",
    "    results[\"shap_explainer\"] = explainer\n",
    "    results[\"shap_values\"] = shap_values\n",
    "\n",
    "    # --- LIME Explanations ---\n",
    "    print(\"üîç Generating LIME explanations for TP, TN, FP, FN...\")\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train.values,\n",
    "        feature_names=attribute_names,\n",
    "        class_names=class_names,\n",
    "        mode=\"classification\"\n",
    "    )\n",
    "\n",
    "    def predict_proba_with_names(x):\n",
    "        return model.predict_proba(pd.DataFrame(x, columns=attribute_names))\n",
    "\n",
    "    lime_figures = []\n",
    "\n",
    "    # Map each (true, pred) pair to a label\n",
    "    lime_categories = {\n",
    "        (1, 1): \"True Positive\",\n",
    "        (0, 0): \"True Negative\",\n",
    "        (0, 1): \"False Positive\",\n",
    "        (1, 0): \"False Negative\"\n",
    "    }\n",
    "\n",
    "    for (true_label, pred_label), category_name in lime_categories.items():\n",
    "        label_name = class_names[true_label]\n",
    "        pred_name = class_names[pred_label]\n",
    "        mask = (y_val == true_label) & (y_val_pred == pred_label)\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            print(f\"‚ö†Ô∏è Skipping category {category_name} ‚Äî no matching examples.\")\n",
    "            continue\n",
    "\n",
    "        sample_indices = y_val[mask].sample(n=min(3, mask.sum()), random_state=42).index\n",
    "\n",
    "        print(f\"\\nüìä LIME: {category_name} (True={label_name}, Pred={pred_name})\")\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            instance = X_val.loc[idx].values\n",
    "            exp = lime_explainer.explain_instance(instance, predict_proba_with_names, num_features=20)\n",
    "            fig = exp.as_pyplot_figure()\n",
    "            fig.suptitle(\n",
    "                f\"LIME | {category_name} | True={label_name} | Pred={pred_name} | Index={idx}\",\n",
    "                fontsize=12\n",
    "            )\n",
    "            lime_figures.append({\n",
    "                \"true_label\": int(true_label),\n",
    "                \"predicted_label\": int(pred_label),\n",
    "                \"true_label_name\": label_name,\n",
    "                \"predicted_label_name\": pred_name,\n",
    "                \"category\": category_name,\n",
    "                \"index\": idx,\n",
    "                \"figure\": fig\n",
    "            })\n",
    "\n",
    "    results[\"lime_explanations\"] = lime_figures\n",
    "\n",
    "    print(\"‚úÖ SHAP and LIME explanations generated.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdf7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should take ~2 minutes to run!\n",
    "purchase_prediction_lightgbm_explanations = generate_binary_classification_model_explanations(purchase_prediction_optuna_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c5b72",
   "metadata": {},
   "source": [
    "## 4.2 SHAP Explanation Insights\n",
    "\n",
    "The SHAP analysis helps us understand which features most strongly influence the model‚Äôs predictions. Here's what we observe from the plots:\n",
    "\n",
    "### 4.2.1 Feature Importance Bar Plot\n",
    "\n",
    "This plot shows the **mean absolute SHAP value** for each feature, indicating its average contribution to the model‚Äôs output ‚Äî regardless of direction (positive or negative):\n",
    "\n",
    "- **Top drivers of model predictions** are:\n",
    "  - `total_clicks`\n",
    "  - `avg_session_duration`\n",
    "  - `impressions_seen`\n",
    "  - `mobile_usage_minutes`\n",
    "  - `social_shares`\n",
    "\n",
    "These are all high-engagement behavioral metrics, suggesting the model is heavily influenced by **user interaction with the platform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_lightgbm_explanations[\"shap_summary_bar_fig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9d7b4",
   "metadata": {},
   "source": [
    "### 4.2.2 SHAP Summary Plot (Beeswarm)\n",
    "\n",
    "This plot adds more nuance:\n",
    "- **Each dot** represents a SHAP value for one row in the validation set.\n",
    "- **Color** reflects the feature value: blue = low, red = high.\n",
    "- **Position on the X-axis** shows whether the feature is pushing the prediction **up** (right) or **down** (left).\n",
    "\n",
    "Key takeaways:\n",
    "- Users with **high `total_clicks`**, `avg_session_duration`, and `mobile_usage_minutes` (red dots on the right) tend to **increase the model's prediction** (likely toward the positive class).\n",
    "- Users with **low values** for these features (blue dots on the left) tend to **decrease** the prediction.\n",
    "- Some features like `prefers_gift_experiences` and `international_tourist` have minor but consistent effects.\n",
    "\n",
    "### 4.2.3 Interpretation Tip\n",
    "\n",
    "> SHAP values are **local explanations**: they show the impact of each feature for each individual prediction. The bar chart gives a global view, but the summary plot helps uncover patterns like: ‚Äúhigh values of X push predictions up.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_lightgbm_explanations[\"shap_summary_fig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b553aa",
   "metadata": {},
   "source": [
    "## 4.3 Interpret LIME Bar Charts\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) helps us understand how a model made a **specific prediction** by approximating its behavior locally with an interpretable linear model.\n",
    "\n",
    "Each LIME explanation is visualized as a **horizontal bar chart**, showing how the input features **push** the prediction toward either class 0 or class 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3.1 What the Bar Chart Shows\n",
    "\n",
    "- Each bar corresponds to a **feature** used in the prediction.\n",
    "- The **length of the bar** represents the **strength of that feature‚Äôs influence** on the prediction.\n",
    "- The **direction** of the bar indicates **which class** the feature is pushing the prediction toward:\n",
    "  - üëâ Right (positive weight): pushes toward class 1 (e.g. \"Purchased\")\n",
    "  - üëà Left (negative weight): pushes toward class 0 (e.g. \"Not Purchased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_lightgbm_explanations[\"lime_explanations\"][0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_prediction_lightgbm_explanations[\"lime_explanations\"][3][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c7f5e",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3.2 Reading the Chart\n",
    "\n",
    "- **Top of the chart**: Features with the strongest influence on the model‚Äôs decision.\n",
    "- **Color**: May vary depending on implementation, but the key is always in **bar direction and length**.\n",
    "- LIME tells a **story for one example** ‚Äî showing which features made the model more confident in its decision, and which features acted against it.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3.3 Why It Matters\n",
    "\n",
    "- LIME helps **debug individual decisions** ‚Äî especially useful for edge cases, errors, or fairness audits.\n",
    "- Combined with SHAP, it provides a full picture: global feature importance (SHAP) + local decision rationale (LIME).\n",
    "\n",
    "> Use LIME to ask: \"Why did the model make *this* prediction for *this* user?\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
