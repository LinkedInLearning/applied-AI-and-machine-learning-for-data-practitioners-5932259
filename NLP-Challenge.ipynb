{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7260a3b2",
   "metadata": {},
   "source": [
    "# Python for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf1fa",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup our Python environment and get started with our Explore California NLP exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e1c66",
   "metadata": {},
   "source": [
    "### Challenge Exercises\n",
    "\n",
    "1. Explore our `locations` NLP dataset\n",
    "2. Implement keyword, TF-IDF, BM-25 and semantic search functionality\n",
    "3. Setup simple Retrival-Augmented-Generation (RAG) AI workflow using a local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd55533",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or hit `command/shift + enter` to run individual cells one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install --quiet rank_bm25==0.2.2\n",
    "!pip install --quiet faiss-cpu==1.11.0\n",
    "!pip install --quiet ctransformers==0.2.27\n",
    "!pip install --quiet dotenv==0.9.9\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259.git data\n",
    "\n",
    "# Need to change directory into \"data\" to download git lfs data objects\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct\n",
    "%cd ..\n",
    "\n",
    "# Turn off future warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b60251",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "We'll begin by calculating the following using our `text_data` column from `locations_df` - this is the equivalent of the structured HTML data we'll extract from the webpage from the Explore California website.\n",
    "\n",
    "* Total count of locations\n",
    "* Vocabulary size and most frequent keywords\n",
    "* Generate a word cloud for `text_data` and compare this with the simplified `description` field\n",
    "* Create sentence embeddings and visualize clusters in 3D space to identify similar locations based off their `descriptions`\n",
    "\n",
    "Let's first load in our `locations` datasets using Pandas and we'll get started by exploring our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f552f",
   "metadata": {},
   "source": [
    "## 1.1 Load Locations Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c51b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load in locations dataset\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "# View first few rows of dataframe\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617dc26",
   "metadata": {},
   "source": [
    "## 1.2 Locations Analysis\n",
    "\n",
    "> How many unique locations are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique locations are there?\n",
    "print(f\"There are {len(locations_df)} unique locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cab075",
   "metadata": {},
   "source": [
    "> How many locations per category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many locations per category?\n",
    "locations_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f2217",
   "metadata": {},
   "source": [
    "> How many locations per region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many locations per region?\n",
    "locations_df[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8d84d",
   "metadata": {},
   "source": [
    "## 1.3 NLP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b76450",
   "metadata": {},
   "source": [
    "### 1.3.1 Most Frequent Terms\n",
    "\n",
    "> Identify the top 25 most frequent terms across all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Identify the Top 25 Most Frequent Terms Across All Locations\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Used for tokenizing and counting word frequencies\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Initialize CountVectorizer\n",
    "# ---------------------------------------\n",
    "# We use uni-gram tokenization (single words) and automatically remove common English stopwords\n",
    "# This helps us focus on meaningful content-specific terms\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Fit the vectorizer to the text data\n",
    "# ---------------------------------------\n",
    "# The input is a list of raw text strings from the 'text_data' column (assumed to be pre-cleaned)\n",
    "# This will tokenize each document and build a term-document matrix\n",
    "X = vectorizer.fit_transform(locations_df['text_data'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Aggregate total term frequencies across all documents\n",
    "# ---------------------------------------\n",
    "# Get the list of all terms (vocabulary)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum up the count of each term across all documents\n",
    "term_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Create a DataFrame showing each term and its total count\n",
    "word_freq = pd.DataFrame({\n",
    "    'term': terms,\n",
    "    'count': term_counts\n",
    "})\n",
    "\n",
    "# Sort terms by descending frequency\n",
    "word_freq = word_freq.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Output results\n",
    "# ---------------------------------------\n",
    "print(f\"Total number of unique uni-gram terms: {len(word_freq)}\")\n",
    "\n",
    "# Display the top 25 most frequent terms\n",
    "word_freq.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb194b2",
   "metadata": {},
   "source": [
    "### 1.3.2 Word Cloud Visualization\n",
    "\n",
    "We'll use our HTML `text_data` and the summary `description` data to build 2 word clouds and compare them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a644435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Compare Frequent Terms in 'text_data' vs 'description' Using Side-by-Side Word Clouds\n",
    "# ---------------------------------------\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Prepare text inputs\n",
    "# ---------------------------------------\n",
    "wordcloud_text_inputs = \" \".join(locations_df['text_data'])\n",
    "wordcloud_description_inputs = \" \".join(locations_df['description'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Generate WordClouds\n",
    "# ---------------------------------------\n",
    "wordcloud_text = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_text_inputs)\n",
    "\n",
    "wordcloud_description = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_description_inputs)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Plot the WordClouds side-by-side\n",
    "# ---------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))  # Create 1 row, 2 column layout\n",
    "\n",
    "# Left: WordCloud for HTML text_data\n",
    "axes[0].imshow(wordcloud_text, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Top Terms in HTML Data\", fontsize=24)\n",
    "\n",
    "# Right: WordCloud for description\n",
    "axes[1].imshow(wordcloud_description, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"Top Terms in Summary Description\", fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6105a0f",
   "metadata": {},
   "source": [
    "### 1.3.2 Embedding and Visualizing Location Descriptions\n",
    "\n",
    "In this exercise, we’re using a lightweight **Sentence Transformer** model (`all-MiniLM-L6-v2`) to turn each location’s `description` into an **embedding** — a numeric representation of its meaning.\n",
    "\n",
    "These models are built using the **Sentence-BERT (SBERT)** architecture, which extends BERT to efficiently produce **sentence-level embeddings** that can be compared using cosine similarity. This allows us to capture semantic meaning — not just exact word overlap — in a compact vector format.\n",
    "\n",
    "We’ll then explore the data using two unsupervised techniques:\n",
    "\n",
    "- **KMeans clustering** groups together descriptions that are semantically similar — think of it as sorting locations by theme.\n",
    "- **t-SNE** helps us reduce the 384-dimensional embeddings down to just 3 dimensions so we can visualize them in a chart.\n",
    "\n",
    "Finally, we plot the results in a **3D interactive Plotly scatter plot**. Each point represents a location, and clusters help reveal patterns in how different descriptions relate to each other.\n",
    "\n",
    "👉 **Tip:** You can click on cluster names in the legend to isolate them, and use your mouse or trackpad to **pan, zoom, and rotate** around the 3D space for a better view.\n",
    "\n",
    "![](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/embedding-workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 3D Visualization of Location Descriptions Using Sentence Embeddings, Clustering, and t-SNE\n",
    "# ---------------------------------------\n",
    "\n",
    "# Import core libraries\n",
    "from sentence_transformers import SentenceTransformer      # For embedding text into dense vector space\n",
    "from sklearn.cluster import KMeans                         # For clustering the text embeddings\n",
    "from sklearn.manifold import TSNE                          # For dimensionality reduction (to 3D)\n",
    "import plotly.express as px                                # For interactive plotting\n",
    "import plotly.io as pio                                    # For controlling plotly renderers in Colab\n",
    "\n",
    "# Ensure that Plotly renders correctly in Google Colab\n",
    "pio.renderers.default = 'colab'\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Encode Descriptions into Embeddings\n",
    "# ---------------------------------------\n",
    "# We use a pre-trained SentenceTransformer model to convert free-text location descriptions\n",
    "# into dense semantic vectors that capture meaning\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(locations_df['description'].tolist())\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Apply KMeans Clustering on the Embeddings\n",
    "# ---------------------------------------\n",
    "# We assign each location to one of 5 semantic clusters using unsupervised learning\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Reduce Embedding Dimensionality to 3D with t-SNE\n",
    "# ---------------------------------------\n",
    "# t-SNE projects high-dimensional embeddings into 3D space for visualization,\n",
    "# preserving local similarity structure\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "embedding_3d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Prepare Data for Plotting\n",
    "# ---------------------------------------\n",
    "# Select only the relevant columns from the original dataset\n",
    "locations_plotting_df = locations_df.loc[:, [\"location_name\", \"description\"]]\n",
    "\n",
    "# Create a shortened version of the description (first 25 characters) for concise tooltips\n",
    "locations_plotting_df[\"short_description\"] = locations_plotting_df['description'].str[:25] + '...'\n",
    "\n",
    "# Assign cluster labels as strings for categorical coloring in the legend\n",
    "locations_plotting_df['cluster'] = 'Cluster ' + clusters.astype(str)\n",
    "\n",
    "# Add the 3D coordinates from the t-SNE projection\n",
    "locations_plotting_df['x'] = embedding_3d[:, 0]\n",
    "locations_plotting_df['y'] = embedding_3d[:, 1]\n",
    "locations_plotting_df['z'] = embedding_3d[:, 2]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 5: Build an Interactive 3D Scatter Plot with Plotly\n",
    "# ---------------------------------------\n",
    "# Each point represents a location, colored by its cluster, and can be explored in 3D\n",
    "fig = px.scatter_3d(\n",
    "    locations_plotting_df,\n",
    "    x='x', y='y', z='z',                      # 3D coordinates\n",
    "    color='cluster',                         # Use cluster for color grouping\n",
    "    hover_data={                             # Tooltip configuration\n",
    "        'location_name': True,\n",
    "        'short_description': True,\n",
    "        'x': False, 'y': False, 'z': False\n",
    "    },\n",
    "    title=\"Embedding Location Descriptions\",\n",
    "    labels={'x': 't-SNE X', 'y': 't-SNE Y'},   # Axis labels for readability\n",
    "    # Order clusters explicitly in the legend\n",
    "    category_orders={'cluster': sorted(locations_plotting_df['cluster'].unique())}\n",
    "\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 6: Format Plot Appearance\n",
    "# ---------------------------------------\n",
    "# Make the chart larger and cleaner with defined width, height, and no axis margins\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Tune the size and transparency of the plot markers\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 7: Render the Interactive Chart\n",
    "# ---------------------------------------\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4367f2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811ed7c",
   "metadata": {},
   "source": [
    "### 1.3.3 Inspecting Embeddings and Plotting Data\n",
    "\n",
    "To better understand what’s happening under the hood, it’s helpful to look at the actual data we’re working with:\n",
    "\n",
    "- **Raw embeddings:** After encoding the descriptions with `SentenceTransformer`, each location is represented as a 384-dimensional vector. These embeddings reflect the semantic meaning of each description and are the foundation for clustering and visualization.\n",
    "\n",
    "- **Plotting DataFrame:** The `locations_plotting_df` DataFrame brings everything together — original fields like `location_name`, the `short_description`, assigned `cluster`, and the 3D coordinates from t-SNE. This structure helps connect the original text with the transformed features used for plotting.\n",
    "\n",
    "Exploring these structures can help you connect the Python code to the actual data transformations — from text, to vectors, to clusters, to 3D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the embedding for Yosemite at index 0\n",
    "print(f\"Raw embedding values for Yosemite National Park (length = {len(embeddings[0])})\")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_plotting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c177b0",
   "metadata": {},
   "source": [
    "# 2. Implementing Search for Explore California\n",
    "\n",
    "In this section, we’ll build out three different **search functionalities** for our case study: **Explore California**. These techniques mirror how a travel website might power its **search experience** — helping users find tours and destinations based on what they type in.\n",
    "\n",
    "We'll explore and compare the following search algorithms:\n",
    "\n",
    "1. **Keyword Search**  \n",
    "   A simple approach that checks if the user’s query appears directly in the text.\n",
    "\n",
    "2. **TF-IDF (Term Frequency–Inverse Document Frequency)**  \n",
    "   A classic information retrieval technique that scores how important a term is in a document relative to the rest of the dataset.\n",
    "\n",
    "3. **BM25 (Best Matching 25)**  \n",
    "   An advanced, ranking-based algorithm that improves on TF-IDF by accounting for term frequency saturation and document length.\n",
    "\n",
    "Once we’ve implemented and compared these traditional search methods, we’ll move on to a more modern approach — performing another round of **embeddings**, this time using a more advanced **Sentence-BERT** model to enable **semantic search** based on meaning rather than just words.\n",
    "\n",
    "This will set the stage for building more intelligent and flexible retrieval systems, similar to what powers modern AI-driven search experiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d9591",
   "metadata": {},
   "source": [
    "## 2.1 Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 🔎 Unified Search Comparison for Explore California\n",
    "# ---------------------------------------\n",
    "# In this cell, we implement 4 search strategies:\n",
    "# 1. Keyword Search\n",
    "# 2. TF-IDF Vector Search\n",
    "# 3. BM25 Ranking\n",
    "# 4. Semantic Search using Sentence-BERT\n",
    "# Each approach is defined as a function so we can easily compare results for the same query.\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer         # For TF-IDF vector search\n",
    "from rank_bm25 import BM25Okapi                                     # For BM25 ranking\n",
    "from sentence_transformers import SentenceTransformer, util        # For semantic search with SBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Prepare the Corpus for Search\n",
    "# ---------------------------------------\n",
    "# We'll search against our detailed HTML data in the text_data field\n",
    "corpus = locations_df[\"text_data\"].fillna('').tolist()\n",
    "\n",
    "# Tokenize for BM25 (required format: list of lists of words)\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Precompute Representations for Search\n",
    "# ---------------------------------------\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25 indexing\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Sentence-BERT embeddings for semantic similarity (this is the same as above!)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Define Reusable Search Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def search_keyword(query, top_k=5):\n",
    "    \"\"\"Return rows that contain the query term (case-insensitive substring match).\"\"\"\n",
    "    results = [i for i, doc in enumerate(corpus) if query.lower() in doc.lower()]\n",
    "    return locations_df.iloc[results][:top_k]\n",
    "\n",
    "def search_tfidf(query, top_k=5):\n",
    "    \"\"\"Rank documents by TF-IDF cosine similarity with the query.\"\"\"\n",
    "    q_vec = tfidf_vectorizer.transform([query])\n",
    "    scores = np.dot(tfidf_matrix, q_vec.T).toarray().ravel()\n",
    "    top_indices = scores.argsort()[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "def search_bm25(query, top_k=5):\n",
    "    \"\"\"Rank documents by BM25 relevance score.\"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "def search_semantic(query, top_k=5):\n",
    "    \"\"\"Rank documents by semantic similarity using Sentence-BERT embeddings.\"\"\"\n",
    "    query_emb = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy()\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Define a Reusable Comparison Function for Search Outputs\n",
    "# ---------------------------------------\n",
    "# This function lets us easily compare the results of all four search methods\n",
    "# — Keyword Match, TF-IDF, BM25, and Semantic SBERT —\n",
    "# for any given query. It's useful for validating how different search techniques\n",
    "# interpret and rank the same input phrase.\n",
    "# ---------------------------------------\n",
    "\n",
    "def compare_search_methods(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Run and display results from all four search methods for a given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search string to evaluate\n",
    "    - top_k (int): Number of top results to return for each method (default = 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keyword Search (exact substring match)\n",
    "    print(f\"\\n🔍 Keyword Search: '{query}'\")\n",
    "    display(search_keyword(query, top_k=top_k))\n",
    "\n",
    "    # TF-IDF Vector Search (weighted match based on term rarity)\n",
    "    print(f\"\\n🧠 TF-IDF Search: '{query}'\")\n",
    "    display(search_tfidf(query, top_k=top_k))\n",
    "\n",
    "    # BM25 Ranking (term frequency-aware ranking algorithm)\n",
    "    print(f\"\\n📚 BM25 Search: '{query}'\")\n",
    "    display(search_bm25(query, top_k=top_k))\n",
    "\n",
    "    # Semantic Search using Sentence-BERT embeddings\n",
    "    print(f\"\\n🤖 Semantic Search (SBERT): '{query}'\")\n",
    "    display(search_semantic(query, top_k=top_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d352b",
   "metadata": {},
   "source": [
    "## 2.2 Comparing Search Methods\n",
    "\n",
    "Now that we’ve implemented all four search strategies — **Keyword**, **TF-IDF**, **BM25**, and **Semantic Search (SBERT)** — we’ll use our `compare_search_methods` function to evaluate how each method handles different types of queries.\n",
    "\n",
    "We’ll start with a few simple, direct queries like:\n",
    "\n",
    "> *“wine tours”*\n",
    "\n",
    "Then move on to more natural, conversational questions such as:\n",
    "\n",
    "> *“Where can I find a good place to workout?”*\n",
    "\n",
    "This comparison helps reveal the strengths and limitations of each approach:\n",
    "\n",
    "- **Keyword Search** is very strict — it only returns results that contain the exact words from the query, so it often misses related or reworded content.\n",
    "- **TF-IDF** and **BM25** offer more flexibility by considering word importance and frequency, though they still rely on exact token overlap.\n",
    "- **Semantic Search (SBERT)** shines on open-ended or vague queries — it can understand meaning even when the wording doesn’t match exactly, making it ideal for natural language search.\n",
    "\n",
    "---\n",
    "\n",
    "👉 **Try it yourself:**  \n",
    "Run the `compare_search_methods()` function with a few of your own queries to see how the results change.  \n",
    "You can also adjust the `top_k` parameter to control how many top matches you want to return (e.g., 3, 5, 10).\n",
    "\n",
    "This is a great way to explore how traditional vs. AI-powered search behaves in a realistic scenario!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be657df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"wine\",\n",
    "  top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21972678",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"wine tours\",\n",
    "  top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"nice beaches near SoCal\",  # SoCal is short for Southern California\n",
    "  top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"Where can I find a good place to workout?\",\n",
    "  top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b9dbb",
   "metadata": {},
   "source": [
    "# 3. Retrieval Augmented Generation LLM Workflow\n",
    "\n",
    "In this next section, we’ll use a **local LLM (TinyLlama)** to answer natural language questions about our Explore California dataset.\n",
    "\n",
    "We’ll try two approaches:\n",
    "\n",
    "1. **Direct Q&A (No Context):**  \n",
    "   First, we’ll ask the model a few questions without providing any background or external knowledge. This helps us see how well a small, locally-run model performs “out of the box.”\n",
    "\n",
    "2. **Contextual Q&A using RAG:**  \n",
    "   Next, we’ll implement a simple **retrieval-augmented generation (RAG)** pipeline using the **semantic embeddings** we created earlier. We’ll:\n",
    "   - Use **`all-MiniLM-L6-v2`**, a Sentence-BERT model, to embed each location’s description into a semantic vector\n",
    "   - Build a **FAISS** index from those embeddings for fast similarity search\n",
    "   - Retrieve the most relevant descriptions for a given query\n",
    "   - Insert those retrieved descriptions into the LLM’s prompt as context\n",
    "   - Ask the same question again — and compare how much better the answers become\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**How we’re running TinyLlama locally via Google Colab:**  \n",
    "We’ll be using the **GGUF version of TinyLlama** provided by [TheBloke](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) on Hugging Face, which is optimized for local inference.  \n",
    "This model runs with the **`ctransformers`** library and supports fast, quantized loading with minimal memory — perfect for running on Google Colab.\n",
    "\n",
    "You’ll specify the quantized model file (e.g., `Q4_K_M`) and load it directly via `AutoModelForCausalLM.from_pretrained()`.\n",
    "\n",
    "---\n",
    "\n",
    "🔍 **Why this matters:**  \n",
    "Local models like TinyLlama are fast and run offline, but they come with limitations — especially around **context size** (the number of tokens they can read at once). This can limit how much background knowledge we can include in a single request.\n",
    "\n",
    "As we move forward, we’ll also explore **cloud-based LLMs** via API calls (like OpenAI, Mistral, or Claude), which support much larger context windows and often produce more accurate and detailed responses — especially for complex or multi-turn questions.\n",
    "\n",
    "This exercise will help you understand the tradeoffs between small local models and larger cloud-hosted ones — and how retrieval can help bridge that gap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd004f93",
   "metadata": {},
   "source": [
    "## 3.1 Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 🧠 Retrieval-Augmented Generation (RAG) using TinyLlama + MiniLM + FAISS\n",
    "# ---------------------------------------\n",
    "# This script compares two approaches to answering questions using a local LLM:\n",
    "# 1. Direct prompt to TinyLlama (no context)\n",
    "# 2. Retrieval-Augmented Generation (RAG) using semantic search + FAISS + TinyLlama\n",
    "#\n",
    "# Key Components:\n",
    "# - SentenceTransformer (MiniLM) for embeddings\n",
    "# - FAISS for fast similarity search\n",
    "# - TinyLlama (GGUF, via ctransformers) for local inference\n",
    "# - Token counting to check prompt size before sending to LLM\n",
    "# ---------------------------------------\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Import dependencies\n",
    "# ---------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import re\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Prepare your dataset\n",
    "# ---------------------------------------\n",
    "# Assumes locations_df has a 'description' column with clean, descriptive text\n",
    "corpus = locations_df[\"description\"].fillna(\"\").tolist()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Generate sentence embeddings using MiniLM\n",
    "# ---------------------------------------\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")                  # Lightweight and fast\n",
    "sbert_embeddings = sbert_model.encode(corpus, convert_to_numpy=True)  # Convert to NumPy for FAISS\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Build a FAISS index for fast similarity search\n",
    "# ---------------------------------------\n",
    "dimension = sbert_embeddings.shape[1]  # 384 for MiniLM\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(sbert_embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Load the TinyLlama local model\n",
    "# ---------------------------------------\n",
    "llama_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    filename=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")\n",
    "\n",
    "tiny_llama_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_path,\n",
    "    model_type=\"llama\",         # Required for TinyLlama GGUF format\n",
    "    gpu_layers=0,               # Set >0 if Colab supports GPU acceleration\n",
    "    max_new_tokens=512          # Increased output length for richer answers\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6. Utility: Estimate prompt token count\n",
    "# ---------------------------------------\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimate token count using basic word+punctuation splitting.\n",
    "    Approximate — useful for checking against 512-token context window.\n",
    "    \"\"\"\n",
    "    return len(re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE))\n",
    "\n",
    "# ---------------------------------------\n",
    "# 7. Define QA response functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def generate_direct_response(query):\n",
    "    \"\"\"\n",
    "    Answer question using TinyLlama with no background knowledge.\n",
    "    Helps evaluate zero-shot performance on local models.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Answer the question as a helpful travel agent based only on your knowledge about California\n",
    "### Question:\\n{query}\\n\\n\n",
    "### Response:\n",
    "    \"\"\"\n",
    "    return tiny_llama_llm(prompt)\n",
    "\n",
    "def generate_rag_response(query, k=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Answer question using RAG: retrieve top-k similar descriptions using FAISS\n",
    "    and inject them as context for TinyLlama to use in its response.\n",
    "    Also prints estimated token count for visibility.\n",
    "    \"\"\"\n",
    "    # Semantic search using query embedding\n",
    "    query_embedding = sbert_model.encode(query, convert_to_numpy=True)\n",
    "    _, top_indices = index.search(query_embedding.reshape(1, -1), k)\n",
    "\n",
    "    # Build context: include both location name and description\n",
    "    context_rows = locations_df.iloc[top_indices[0]]\n",
    "    context = \"\\n\".join(\n",
    "        f\"{row['location_name']}: {row['description']}\" for _, row in context_rows.iterrows()\n",
    "    )\n",
    "\n",
    "    # Construct a formatted prompt with context and question\n",
    "    prompt = f\"\"\"Answer the question as a helpful travel agent using only the provided context.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    # Estimate tokens in the prompt\n",
    "    prompt_tokens = count_tokens(prompt)\n",
    "    if verbose:\n",
    "        print(f\"📏 Estimated tokens in prompt: {prompt_tokens}\")\n",
    "    \n",
    "    # Stop if token limit exceeded\n",
    "    if prompt_tokens > 512:\n",
    "        raise ValueError(\"🚫 Prompt exceeds TinyLlama's 512-token context limit. Try reducing `k` or trimming the context.\")\n",
    "\n",
    "    # Generate response using TinyLlama and return as dict along with context\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": tiny_llama_llm(prompt)\n",
    "    }\n",
    "\n",
    "# ---------------------------------------\n",
    "# 8. Compare Direct vs RAG-Enhanced Responses\n",
    "# ---------------------------------------\n",
    "\n",
    "# This is a convenience function to print long strings into multiple lines\n",
    "def wrap_print(text):\n",
    "    print(textwrap.fill(text, width=80))\n",
    "\n",
    "def compare_llm_responses(query, k=3):\n",
    "    \"\"\"\n",
    "    Compare TinyLlama's response to a query with and without RAG-enhanced context.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The natural language question to ask the model\n",
    "    - k (int): Number of top matching documents to retrieve for RAG\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\n🧠 Query: {query}\")\n",
    "    \n",
    "    # 🔹 Step 1: Run Direct Q&A with no background context\n",
    "    print(\"\\n🤖 Direct LLM Response (No Context Provided):\")\n",
    "    direct_response = generate_direct_response(query)\n",
    "    wrap_print(direct_response)\n",
    "    \n",
    "    # 🔹 Step 2: Run RAG-enhanced Q&A\n",
    "    print(\"\\n📡 Running Retrieval-Augmented Generation (RAG)...\")\n",
    "    rag_response = generate_rag_response(query, k=k)\n",
    "\n",
    "    # 🔹 Step 3: Show enhanced prompt that will be fed into the model\n",
    "    print(f\"\\n📚 Enhanced Prompt With Retrieved Context (Top {k} Documents):\")\n",
    "    print(rag_response[\"prompt\"])\n",
    "\n",
    "    # 🔹 Step 4: Show the final model response with RAG\n",
    "    print(\"\\n💬 RAG-Enhanced Response (With Context):\")\n",
    "    wrap_print(rag_response[\"response\"])\n",
    "    print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bab894",
   "metadata": {},
   "source": [
    "### 3.2 Comparing Simple vs Conversational Queries\n",
    "\n",
    "Now that we’ve set up both direct querying and our RAG (retrieval-augmented generation) pipeline, it’s time to test how well they perform across different types of natural language queries.\n",
    "\n",
    "We’ll start with **simple, keyword-style queries** — like `\"scenic hikes\"` or `\"wine tours\"` — that resemble what a user might type into a traditional search box.\n",
    "\n",
    "Then we’ll move on to more **conversational, open-ended questions**, such as:\n",
    "\n",
    "- `\"Where can I go kayaking or canoeing in California?\"`\n",
    "- `\"What are some good places for stargazing?\"`\n",
    "- `\"I'm traveling with kids — any family-friendly adventures?\"`\n",
    "\n",
    "By comparing the answers returned by:\n",
    "\n",
    "- A **direct local LLM (TinyLlama)** without any context, and  \n",
    "- The same model enhanced with **retrieved context from FAISS**,  \n",
    "\n",
    "we’ll get a better sense of **how retrieval helps smaller models** understand and respond more accurately — especially as the queries become more natural and less keyword-focused.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Important Notes:**\n",
    "\n",
    "- **Responses may vary slightly each time you run them.**\n",
    "- **Token limits matter.** If the prompt (context + question) exceeds TinyLlama’s 512-token limit, it may raise an error or truncate output.\n",
    "- **💡 Recommendation:** Keep `top_k` at or below **5** to avoid exceeding the token limit.  \n",
    "  If you include too many documents in the prompt, the model may not be able to read the full context effectively.\n",
    "- **Try it yourself:**  \n",
    "  - Experiment by entering your own questions  \n",
    "  - Adjust the `top_k` parameter to include more or fewer context items  \n",
    "  - Re-run the same query to see how the results might vary across runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"scenic hikes\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2120c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"wine tours\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"Where can I go kayaking or canoeing in California?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d796586",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"What are some good places for stargazing?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"I'm traveling with kids — any family-friendly adventures?\", k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
