{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7260a3b2",
   "metadata": {},
   "source": [
    "# Python for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf1fa",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup our Python environment and get started with our Explore California NLP exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e1c66",
   "metadata": {},
   "source": [
    "### Challenge Exercises\n",
    "\n",
    "1. Explore our `locations` NLP dataset\n",
    "2. Implement keyword, TF-IDF and BM-25 search functionality\n",
    "3. Create `BERT` sentence embeddings and implement semantic search\n",
    "4. Setup simple Retrival-Augmented-Generation (RAG) AI workflow\n",
    "\n",
    "We'll also cover how to use cloud-based LLMs to make **100% free** API calls to various LLMs using `OpenAI` and `OpenRouter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd55533",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or hit `command/shift + enter` to run individual cells one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install --quiet rank_bm25==0.2.2\n",
    "!pip install --quiet faiss-cpu==1.11.0\n",
    "!pip install --quiet ctransformers==0.2.27\n",
    "!pip install --quiet dotenv==0.9.9\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259.git data\n",
    "\n",
    "# Need to change directory into \"data\" to download git lfs data objects\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b60251",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "We'll begin by calculating the following using our `text_data` column from `locations_df` - this is the equivalent of the structured HTML data we'll extract from the webpage from the Explore California website.\n",
    "\n",
    "* Total count of locations\n",
    "* Vocabulary size and most frequent keywords\n",
    "* Generate a word cloud for `text_data` and compare this with the simplified `description` field\n",
    "* Create sentence embeddings and visualize clusters in 3D space to identify similar locations based off their `descriptions`\n",
    "\n",
    "Let's first load in our `locations` datasets using Pandas and we'll get started by exploring our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f552f",
   "metadata": {},
   "source": [
    "## 1.1 Load Locations Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c51b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load in locations dataset\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "# View first few rows of dataframe\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617dc26",
   "metadata": {},
   "source": [
    "## 1.2 Locations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique locations are there?\n",
    "print(f\"There are {len(locations_df)} unique locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the locations by region\n",
    "locations_df[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the locations by category\n",
    "locations_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8d84d",
   "metadata": {},
   "source": [
    "## 1.3 NLP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b76450",
   "metadata": {},
   "source": [
    "### 1.3.1 Most Frequent Terms\n",
    "\n",
    "> Identify the top 25 most frequent terms across all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Identify the Top 25 Most Frequent Terms Across All Locations\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Used for tokenizing and counting word frequencies\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Initialize CountVectorizer\n",
    "# ---------------------------------------\n",
    "# We use uni-gram tokenization (single words) and automatically remove common English stopwords\n",
    "# This helps us focus on meaningful content-specific terms\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Fit the vectorizer to the text data\n",
    "# ---------------------------------------\n",
    "# The input is a list of raw text strings from the 'text_data' column (assumed to be pre-cleaned)\n",
    "# This will tokenize each document and build a term-document matrix\n",
    "X = vectorizer.fit_transform(locations_df['text_data'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Aggregate total term frequencies across all documents\n",
    "# ---------------------------------------\n",
    "# Get the list of all terms (vocabulary)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum up the count of each term across all documents\n",
    "term_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Create a DataFrame showing each term and its total count\n",
    "word_freq = pd.DataFrame({\n",
    "    'term': terms,\n",
    "    'count': term_counts\n",
    "})\n",
    "\n",
    "# Sort terms by descending frequency\n",
    "word_freq = word_freq.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Output results\n",
    "# ---------------------------------------\n",
    "print(f\"Total number of unique uni-gram terms: {len(word_freq)}\")\n",
    "\n",
    "# Display the top 25 most frequent terms\n",
    "word_freq.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb194b2",
   "metadata": {},
   "source": [
    "### 1.3.2 Word Cloud Visualization\n",
    "\n",
    "We'll use our HTML `text_data` and the summary `description` data to build 2 word clouds and compare them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a644435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Compare Frequent Terms in 'text_data' vs 'description' Using Side-by-Side Word Clouds\n",
    "# ---------------------------------------\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Prepare text inputs\n",
    "# ---------------------------------------\n",
    "wordcloud_text_inputs = \" \".join(locations_df['text_data'])\n",
    "wordcloud_description_inputs = \" \".join(locations_df['description'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Generate WordClouds\n",
    "# ---------------------------------------\n",
    "wordcloud_text = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_text_inputs)\n",
    "\n",
    "wordcloud_description = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_description_inputs)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Plot the WordClouds side-by-side\n",
    "# ---------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))  # Create 1 row, 2 column layout\n",
    "\n",
    "# Left: WordCloud for HTML text_data\n",
    "axes[0].imshow(wordcloud_text, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Top Terms in HTML Data\", fontsize=24)\n",
    "\n",
    "# Right: WordCloud for description\n",
    "axes[1].imshow(wordcloud_description, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"Top Terms in Summary Description\", fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6105a0f",
   "metadata": {},
   "source": [
    "### 1.3.2 Sentence Embeddings\n",
    "\n",
    "In this final exploration exercise - we'll use a light sentence transformer model to encode our summary `description` data as embeddings.\n",
    "\n",
    "Embeddings is a term that gets used quite a lot in machine learning and AI so it's useful to understand how it works from a basic perspective.\n",
    "\n",
    "![](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/embedding-workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer  # For encoding text into embeddings\n",
    "from sklearn.cluster import KMeans                     # For unsupervised clustering\n",
    "from sklearn.manifold import TSNE                      # For dimensionality reduction to 3D\n",
    "import plotly.express as px                            # For interactive 3D plotting\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Encode location descriptions into embeddings\n",
    "# ---------------------------------------\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Load a pre-trained lightweight sentence embedding model\n",
    "embeddings = embedding_model.encode(locations_df['description'].tolist())  # Generate embeddings for each description\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Cluster embeddings into semantic groups\n",
    "# ---------------------------------------\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  # Define k-means clustering with 5 clusters\n",
    "clusters = kmeans.fit_predict(embeddings)       # Assign each embedding to a cluster\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Reduce high-dimensional embeddings to 3D for visualization\n",
    "# ---------------------------------------\n",
    "tsne = TSNE(n_components=3, random_state=42)     # Initialize t-SNE for 3D projection\n",
    "embedding_3d = tsne.fit_transform(embeddings)    # Perform dimensionality reduction\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Prepare the DataFrame for plotting\n",
    "# ---------------------------------------\n",
    "locations_plotting_df = locations_df.loc[:, [\"location_name\", \"description\"]]  # Select relevant columns\n",
    "\n",
    "# Create a shortened version of the description for tooltips (first 25 characters + ellipsis)\n",
    "locations_plotting_df[\"short_description\"] = locations_plotting_df['description'].str[:25] + '...'\n",
    "\n",
    "# Add clustering and 3D coordinates to the plotting DataFrame\n",
    "locations_plotting_df['cluster'] = clusters\n",
    "locations_plotting_df['x'] = embedding_3d[:, 0]\n",
    "locations_plotting_df['y'] = embedding_3d[:, 1]\n",
    "locations_plotting_df['z'] = embedding_3d[:, 2]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 5: Create an interactive 3D scatter plot\n",
    "# ---------------------------------------\n",
    "fig = px.scatter_3d(\n",
    "    locations_plotting_df,\n",
    "    x='x', y='y', z='z',              # 3D coordinates from t-SNE\n",
    "    color='cluster',                 # Color by assigned cluster\n",
    "    hover_data={                     # Customize tooltip content\n",
    "        'location_name': True,\n",
    "        'short_description': True,\n",
    "        'x': False, 'y': False, 'z': False  # Hide raw coordinates in tooltip\n",
    "    },\n",
    "    title=\"Embedding Location Descriptions\",\n",
    "    labels={'x': 't-SNE X', 'y': 't-SNE Y'}  # Axis labels\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 6: Customize figure appearance\n",
    "# ---------------------------------------\n",
    "fig.update_layout(\n",
    "    width=1000, height=1000,                   # Set plot size\n",
    "    margin=dict(l=0, r=0, b=0, t=40)           # Adjust margins for cleaner layout\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))  # Tune marker size and transparency\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 7: Show the interactive plot\n",
    "# ---------------------------------------\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4367f2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_plotting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee81e5c",
   "metadata": {},
   "source": [
    "## Setup Cloud LLMs API Keys\n",
    "\n",
    "In the cell below - please add your `Open AI` and `OpenRouter` API keys to make inference calls to our cloud based LLMs.\n",
    "\n",
    "Open AI has a free $5-10 trial available for new users and OpenRouter has a limit of 50 API calls per day which should be more than enough for our initial testing purposes!\n",
    "\n",
    "> **Warning ðŸš¨ðŸš¨ðŸš¨** Make sure to never share your API keys and be careful to expose them to the public!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "OPEN_AI_API_KEY=UPDATE-YOUR-API-KEY-HERE\n",
    "OPEN_ROUTER_API_KEY=UPDATE-YOUR-API-KEY-HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b07f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Save the token as local variable\n",
    "open_ai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "print(\"Open AI API key loaded:\", open_ai_api_key[:8] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d936e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the .env file\n",
    "if os.path.exists(\".env\"):\n",
    "    os.remove(\".env\")\n",
    "    print(\"âœ… .env file removed safely.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
